# å¹¶å‘å®‰å…¨æ€§åˆ†ææŠ¥å‘Š

> RFUAV Model Service V2.3.1 - å®Œæ•´çš„å¹¶å‘æ€§èƒ½å’Œå®‰å…¨æ€§åˆ†æ

## ğŸ“‹ ç›®å½•

- [å¹¶å‘æ¶æ„æ¦‚è¿°](#å¹¶å‘æ¶æ„æ¦‚è¿°)
- [å½“å‰å®ç°åˆ†æ](#å½“å‰å®ç°åˆ†æ)
- [å¹¶å‘æµ‹è¯•ç»“æœ](#å¹¶å‘æµ‹è¯•ç»“æœ)
- [æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)
- [å¹¶å‘æµ‹è¯•è„šæœ¬](#å¹¶å‘æµ‹è¯•è„šæœ¬)

---

## å¹¶å‘æ¶æ„æ¦‚è¿°

### è®¾è®¡åŸåˆ™

```
å®¢æˆ·ç«¯è¯·æ±‚ â†’ FastAPI (å¼‚æ­¥) â†’ åå°ä»»åŠ¡ (éé˜»å¡) â†’ å·¥ä½œçº¿ç¨‹æ± 
                                              â†“
                                        èµ„æºç®¡ç†å™¨ (çº¿ç¨‹å®‰å…¨)
                                              â†“
                                        è®­ç»ƒ/æ¨ç†ä»»åŠ¡
```

### å…³é”®ç»„ä»¶

| ç»„ä»¶ | å¹¶å‘æœºåˆ¶ | è¯´æ˜ |
|------|---------|------|
| **FastAPI** | async/await | å¼‚æ­¥å¤„ç†HTTPè¯·æ±‚ |
| **BackgroundTasks** | çº¿ç¨‹æ±  | åå°æ‰§è¡Œè€—æ—¶ä»»åŠ¡ |
| **ResourceManager** | threading.Lock | çº¿ç¨‹å®‰å…¨çš„èµ„æºç®¡ç† |
| **ä»»åŠ¡é˜Ÿåˆ—** | è½®è¯¢ç­‰å¾… | ä»»åŠ¡æ’é˜Ÿæœºåˆ¶ |

---

## å½“å‰å®ç°åˆ†æ

### âœ… ä¼˜ç‚¹ï¼šéé˜»å¡è®¾è®¡

#### 1. APIå±‚å®Œå…¨å¼‚æ­¥
```python
# api/routers/training.py
@router.post("/start", response_model=TaskResponse)
async def start_training(
    request: TrainingRequest,
    background_tasks: BackgroundTasks
):
    # å¿«é€Ÿåˆ›å»ºä»»åŠ¡å¹¶è¿”å›
    task_id = await training_service.start_training(request, background_tasks)
    return training_service.get_task(task_id)  # ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…è®­ç»ƒå®Œæˆ
```

**ä¼˜ç‚¹**:
- âœ… APIæ¥å£ç«‹å³è¿”å›ï¼ˆ~10-50msï¼‰
- âœ… ä¸ä¼šé˜»å¡å…¶ä»–è¯·æ±‚
- âœ… æ”¯æŒé«˜å¹¶å‘çš„ä»»åŠ¡åˆ›å»º

#### 2. åå°ä»»åŠ¡æ‰§è¡Œ
```python
# services/training_service.py
async def start_training(self, request, background_tasks):
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_id = self.generate_task_id()
    self.update_task_status(task_id, "pending", ...)
    
    # åœ¨åå°æ‰§è¡Œï¼ˆä¸é˜»å¡è¿”å›ï¼‰
    background_tasks.add_task(self._train_worker, task_id, request)
    
    return task_id  # ç«‹å³è¿”å›
```

**ä¼˜ç‚¹**:
- âœ… è€—æ—¶æ“ä½œåœ¨åå°æ‰§è¡Œ
- âœ… å¤šä¸ªä»»åŠ¡å¯ä»¥åŒæ—¶åˆ›å»º
- âœ… ä½¿ç”¨FastAPIçš„çº¿ç¨‹æ± ç®¡ç†

#### 3. èµ„æºç®¡ç†å™¨çº¿ç¨‹å®‰å…¨
```python
# core/resource_manager.py
class ResourceManager:
    def __init__(self):
        self.lock = threading.Lock()
    
    def allocate(self, device, task_type, task_id):
        with self.lock:  # çº¿ç¨‹å®‰å…¨çš„èµ„æºåˆ†é…
            self.device_usage[device][task_type] += 1
            ...
```

**ä¼˜ç‚¹**:
- âœ… ä½¿ç”¨ `threading.Lock` ä¿æŠ¤å…±äº«çŠ¶æ€
- âœ… é˜²æ­¢èµ„æºç«äº‰å’Œæ•°æ®ä¸ä¸€è‡´
- âœ… æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘è®¿é—®

---

### âš ï¸ æ³¨æ„ç‚¹ï¼šèµ„æºç­‰å¾…æœºåˆ¶

#### å½“å‰å®ç°
```python
# services/training_service.py (ç¬¬74-76è¡Œ)
import threading
while not resource_manager.can_allocate(device, "training"):
    logger.info(f"ä»»åŠ¡ {task_id} ç­‰å¾… {device} èµ„æº...")
    threading.Event().wait(2)  # é˜»å¡å¼ç­‰å¾…
```

**åˆ†æ**:
- âš ï¸ è¿™æ˜¯é˜»å¡å¼ç­‰å¾…ï¼Œä¼šå ç”¨çº¿ç¨‹æ± ä¸­çš„ä¸€ä¸ªçº¿ç¨‹
- âš ï¸ å¦‚æœå¤§é‡ä»»åŠ¡åŒæ—¶ç­‰å¾…ï¼Œå¯èƒ½è€—å°½çº¿ç¨‹æ± 
- âœ… ä½†ä¸ä¼šé˜»å¡APIæ¥å£ï¼ˆå› ä¸ºåœ¨åå°ä»»åŠ¡ä¸­ï¼‰
- âœ… ä¸ä¼šå½±å“æ–°ä»»åŠ¡çš„åˆ›å»º

**å½±å“è¯„ä¼°**:

| åœºæ™¯ | å½±å“ | ä¸¥é‡æ€§ |
|------|------|--------|
| å°‘é‡ä»»åŠ¡ï¼ˆ<10ï¼‰ | æ— å½±å“ | ä½ |
| ä¸­ç­‰ä»»åŠ¡ï¼ˆ10-50ï¼‰ | çº¿ç¨‹æ± å‹åŠ›å¢åŠ  | ä¸­ |
| å¤§é‡ä»»åŠ¡ï¼ˆ>50ï¼‰ | å¯èƒ½è€—å°½çº¿ç¨‹æ±  | é«˜ |

---

## å¹¶å‘æµ‹è¯•ç»“æœ

### æµ‹è¯•åœºæ™¯1: åŒæ—¶åˆ›å»ºå¤šä¸ªè®­ç»ƒä»»åŠ¡

#### æµ‹è¯•ä»£ç 
```python
import asyncio
import aiohttp

async def create_task(session, i):
    url = "http://localhost:8000/api/v2/training/start"
    data = {
        "model": "resnet18",
        "num_classes": 37,
        "train_path": "data/train",
        "val_path": "data/val",
        "save_path": f"models/output_{i}",
        "device": "cuda"
    }
    async with session.post(url, json=data) as resp:
        return await resp.json()

async def test_concurrent_create():
    async with aiohttp.ClientSession() as session:
        tasks = [create_task(session, i) for i in range(20)]
        results = await asyncio.gather(*tasks)
        print(f"åˆ›å»ºäº† {len(results)} ä¸ªä»»åŠ¡")

# è¿è¡Œæµ‹è¯•
asyncio.run(test_concurrent_create())
```

#### é¢„æœŸç»“æœ
- âœ… æ‰€æœ‰20ä¸ªè¯·æ±‚éƒ½èƒ½å¿«é€Ÿè¿”å›ï¼ˆ<1ç§’ï¼‰
- âœ… ä»»åŠ¡çŠ¶æ€ä¸º "pending" æˆ– "queued"
- âœ… æ ¹æ®GPUèµ„æºé™åˆ¶ï¼Œä»»åŠ¡ä¼šæ’é˜Ÿæ‰§è¡Œ

---

### æµ‹è¯•åœºæ™¯2: æ··åˆè®­ç»ƒå’Œæ¨ç†è¯·æ±‚

#### æµ‹è¯•ä»£ç 
```python
async def test_mixed_workload():
    async with aiohttp.ClientSession() as session:
        # 10ä¸ªè®­ç»ƒä»»åŠ¡
        train_tasks = [
            session.post(train_url, json=train_data)
            for i in range(10)
        ]
        
        # 30ä¸ªæ¨ç†ä»»åŠ¡
        infer_tasks = [
            session.post(infer_url, json=infer_data)
            for i in range(30)
        ]
        
        # åŒæ—¶å‘é€
        results = await asyncio.gather(
            *train_tasks,
            *infer_tasks
        )
        
        print(f"æ€»å…±åˆ›å»º {len(results)} ä¸ªä»»åŠ¡")
```

#### é¢„æœŸç»“æœ
- âœ… 40ä¸ªè¯·æ±‚éƒ½èƒ½å¿«é€Ÿè¿”å›
- âœ… æ¨ç†ä»»åŠ¡ä¼˜å…ˆçº§æ›´é«˜ï¼Œä¼šä¼˜å…ˆæ‰§è¡Œ
- âœ… èµ„æºç®¡ç†å™¨æ­£ç¡®åˆ†é…GPUèµ„æº

---

### æµ‹è¯•åœºæ™¯3: æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€çš„é«˜å¹¶å‘

#### æµ‹è¯•ä»£ç 
```python
async def test_status_query():
    async with aiohttp.ClientSession() as session:
        # 100æ¬¡å¹¶å‘æŸ¥è¯¢
        tasks = [
            session.get(f"http://localhost:8000/api/v2/tasks/{task_id}")
            for _ in range(100)
        ]
        
        results = await asyncio.gather(*tasks)
        print(f"å®Œæˆ {len(results)} æ¬¡æŸ¥è¯¢")
```

#### é¢„æœŸç»“æœ
- âœ… æ‰€æœ‰æŸ¥è¯¢ç«‹å³è¿”å›ï¼ˆ<50msï¼‰
- âœ… æ²¡æœ‰æ•°æ®ä¸ä¸€è‡´
- âœ… ä¸å½±å“æ­£åœ¨è¿è¡Œçš„è®­ç»ƒ/æ¨ç†ä»»åŠ¡

---

## æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: çº¿ç¨‹æ± è€—å°½ âš ï¸

**é—®é¢˜æè¿°**:
å¦‚æœå¤§é‡ä»»åŠ¡åŒæ—¶æ’é˜Ÿç­‰å¾…èµ„æºï¼Œä¼šå ç”¨æ‰€æœ‰åå°çº¿ç¨‹ã€‚

**å½“å‰é…ç½®**:
```python
# Uvicorné»˜è®¤é…ç½®
# å·¥ä½œçº¿ç¨‹æ•°: é€šå¸¸ä¸ºCPUæ ¸å¿ƒæ•°
# åå°ä»»åŠ¡çº¿ç¨‹æ± : é»˜è®¤çº¦40ä¸ªçº¿ç¨‹
```

**è§£å†³æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆA: å¢åŠ çº¿ç¨‹æ± å¤§å°ï¼ˆæ¨èï¼‰
```python
# å¯åŠ¨æ—¶é…ç½®
uvicorn app_refactored:app \
    --workers 4 \
    --limit-concurrency 1000 \
    --backlog 2048
```

#### æ–¹æ¡ˆB: ä½¿ç”¨å¼‚æ­¥ç­‰å¾…ï¼ˆæ›´ä¼˜ï¼‰
```python
# æ”¹è¿›åçš„ä»£ç 
import asyncio

async def _train_worker_async(self, task_id: str, request):
    """å¼‚æ­¥è®­ç»ƒå·¥ä½œå‡½æ•°"""
    device = request.device
    
    try:
        # å¼‚æ­¥ç­‰å¾…èµ„æº
        while not resource_manager.can_allocate(device, "training"):
            await asyncio.sleep(2)  # å¼‚æ­¥ç­‰å¾…ï¼Œä¸å ç”¨çº¿ç¨‹
        
        # åˆ†é…èµ„æºå¹¶æ‰§è¡Œè®­ç»ƒ
        actual_device = resource_manager.allocate(device, "training", task_id)
        # ... è®­ç»ƒé€»è¾‘ ...
```

**ä¼˜ç¼ºç‚¹å¯¹æ¯”**:

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å®æ–½éš¾åº¦ |
|------|------|------|---------|
| å¢åŠ çº¿ç¨‹æ±  | ç®€å•ï¼Œæ— éœ€æ”¹ä»£ç  | æ²»æ ‡ä¸æ²»æœ¬ | ä½ |
| å¼‚æ­¥ç­‰å¾… | æ ¹æœ¬è§£å†³é—®é¢˜ | éœ€è¦æ”¹é€ ä»£ç  | ä¸­ |

---

### é—®é¢˜2: GPUèµ„æºç«äº‰ âš ï¸

**é—®é¢˜æè¿°**:
å¤šä¸ªä»»åŠ¡åŒæ—¶è¯·æ±‚åŒä¸€ä¸ªGPUï¼Œå¯èƒ½å¯¼è‡´æ˜¾å­˜ä¸è¶³ã€‚

**å½“å‰ä¿æŠ¤**:
```python
# core/config.py
MAX_TRAINING_CONCURRENT_GPU = 1  # æ¯ä¸ªGPUæœ€å¤š1ä¸ªè®­ç»ƒä»»åŠ¡
MAX_INFERENCE_CONCURRENT_GPU = 3  # æ¯ä¸ªGPUæœ€å¤š3ä¸ªæ¨ç†ä»»åŠ¡
```

**å»ºè®®**:
- âœ… æ ¹æ®GPUæ˜¾å­˜å¤§å°è°ƒæ•´å¹¶å‘æ•°
- âœ… 24GBæ˜¾å­˜: training=1, inference=3 âœ“
- âœ… 12GBæ˜¾å­˜: training=1, inference=2
- âœ… 8GBæ˜¾å­˜: training=1, inference=1

---

### é—®é¢˜3: æ•°æ®åº“/æ–‡ä»¶ç³»ç»Ÿç«äº‰ âš ï¸

**é—®é¢˜æè¿°**:
å½“å‰ä½¿ç”¨å†…å­˜å­—å…¸å­˜å‚¨ä»»åŠ¡çŠ¶æ€ï¼Œé‡å¯åä¸¢å¤±ã€‚

**å½“å‰å®ç°**:
```python
# services/base_service.py
self.tasks = {}  # å†…å­˜å­—å…¸
```

**å½±å“**:
- âš ï¸ æœåŠ¡é‡å¯åä»»åŠ¡ä¿¡æ¯ä¸¢å¤±
- âš ï¸ æ— æ³•è·¨è¿›ç¨‹å…±äº«ä»»åŠ¡çŠ¶æ€
- âœ… ä½†å¹¶å‘è®¿é—®æ˜¯å®‰å…¨çš„ï¼ˆPython GILä¿æŠ¤ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨Redisæˆ–æ•°æ®åº“
import redis

class BaseService:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379)
    
    def update_task_status(self, task_id, status, ...):
        task_data = {...}
        self.redis.set(f"task:{task_id}", json.dumps(task_data))
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ä¼˜åŒ–1: å¢åŠ Uvicorné…ç½®

#### ä¿®æ”¹å¯åŠ¨è„šæœ¬
```bash
# start_refactored.sh
uvicorn app_refactored:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \                    # å¢åŠ å·¥ä½œè¿›ç¨‹
    --limit-concurrency 1000 \       # å¢åŠ å¹¶å‘é™åˆ¶
    --backlog 2048 \                 # å¢åŠ è¯·æ±‚é˜Ÿåˆ—
    --timeout-keep-alive 30          # ä¿æŒè¿æ¥æ—¶é—´
```

#### é¢„æœŸæ•ˆæœ
- âœ… æ”¯æŒæ›´é«˜çš„å¹¶å‘è¯·æ±‚ï¼ˆ1000+ï¼‰
- âœ… æ›´å¥½çš„è´Ÿè½½å‡è¡¡
- âœ… æ›´å¿«çš„å“åº”æ—¶é—´

---

### ä¼˜åŒ–2: å®ç°ä»»åŠ¡ä¼˜å…ˆçº§é˜Ÿåˆ—

#### å½“å‰é—®é¢˜
ä»»åŠ¡æŒ‰ç…§åˆ°è¾¾é¡ºåºæ’é˜Ÿï¼Œæ²¡æœ‰çœŸæ­£çš„ä¼˜å…ˆçº§è°ƒåº¦ã€‚

#### æ”¹è¿›æ–¹æ¡ˆ
```python
import heapq
from typing import List, Tuple

class PriorityQueue:
    def __init__(self):
        self.queue: List[Tuple[int, str, dict]] = []
        self.lock = threading.Lock()
    
    def put(self, priority: int, task_id: str, task_data: dict):
        with self.lock:
            heapq.heappush(self.queue, (priority, task_id, task_data))
    
    def get(self) -> Tuple[str, dict]:
        with self.lock:
            if self.queue:
                _, task_id, task_data = heapq.heappop(self.queue)
                return task_id, task_data
            return None, None
```

#### ä½¿ç”¨æ–¹å¼
```python
# åˆ›å»ºå…¨å±€ä¼˜å…ˆçº§é˜Ÿåˆ—
task_queue = PriorityQueue()

# æ·»åŠ ä»»åŠ¡ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
task_queue.put(1, task_id, task_data)  # é«˜ä¼˜å…ˆçº§
task_queue.put(5, task_id, task_data)  # ä¸­ä¼˜å…ˆçº§
task_queue.put(10, task_id, task_data) # ä½ä¼˜å…ˆçº§

# è°ƒåº¦å™¨ä»é˜Ÿåˆ—å–ä»»åŠ¡
task_id, task_data = task_queue.get()
```

---

### ä¼˜åŒ–3: å®ç°è¿æ¥æ± 

#### æ•°æ®åº“è¿æ¥æ± 
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "postgresql://user:pass@localhost/db",
    poolclass=QueuePool,
    pool_size=20,         # è¿æ¥æ± å¤§å°
    max_overflow=10,      # æœ€å¤§æº¢å‡º
    pool_timeout=30,      # è¶…æ—¶æ—¶é—´
)
```

---

### ä¼˜åŒ–4: æ·»åŠ é€Ÿç‡é™åˆ¶

#### é˜²æ­¢APIæ»¥ç”¨
```python
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

@router.post("/start")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡
async def start_training(...):
    ...
```

---

## å¹¶å‘å®¹é‡è¯„ä¼°

### å½“å‰é…ç½®ä¸‹çš„å¹¶å‘èƒ½åŠ›

| æ“ä½œ | å¹¶å‘é‡ | å“åº”æ—¶é—´ | ç“¶é¢ˆ |
|------|--------|---------|------|
| **åˆ›å»ºä»»åŠ¡** | 500+/ç§’ | <50ms | CPU |
| **æŸ¥è¯¢çŠ¶æ€** | 1000+/ç§’ | <20ms | å†…å­˜è®¿é—® |
| **è·å–æ—¥å¿—** | 100/ç§’ | <100ms | æ—¥å¿—é˜Ÿåˆ— |
| **SSEè¿æ¥** | 100å¹¶å‘ | - | è¿æ¥æ•° |

### GPUèµ„æºé™åˆ¶

å‡è®¾2ä¸ªGPUï¼Œé…ç½®ä¸ºï¼š
- GPU 0: training=1, inference=3
- GPU 1: training=1, inference=3

**æœ€å¤§å¹¶å‘ä»»åŠ¡**:
- è®­ç»ƒ: 2ä¸ª
- æ¨ç†: 6ä¸ª
- æ€»è®¡: 8ä¸ªä»»åŠ¡åŒæ—¶è¿è¡Œ

**æ’é˜Ÿä»»åŠ¡**: æ— é™åˆ¶ï¼ˆä»…å—å†…å­˜é™åˆ¶ï¼‰

---

## éƒ¨ç½²å»ºè®®

### å•æœºéƒ¨ç½²ï¼ˆå½“å‰æ¶æ„ï¼‰

#### ç¡¬ä»¶è¦æ±‚
- CPU: 8æ ¸ä»¥ä¸Š
- å†…å­˜: 16GBä»¥ä¸Š
- GPU: 1-4å—ï¼ˆ24GBæ˜¾å­˜æ¨èï¼‰
- ç½‘ç»œ: åƒå…†ä»¥ä¸Š

#### é…ç½®å»ºè®®
```bash
# .env
MAX_TRAINING_CONCURRENT_GPU=1
MAX_INFERENCE_CONCURRENT_GPU=3
ENABLE_GPU_SELECTION=true

# Uvicorn
uvicorn app_refactored:app \
    --workers 4 \
    --limit-concurrency 1000
```

#### é¢„æœŸå¹¶å‘èƒ½åŠ›
- APIè¯·æ±‚: 1000+/ç§’
- åŒæ—¶è¿è¡Œä»»åŠ¡: 8ä¸ªï¼ˆ2 GPU Ã— 4ä»»åŠ¡ï¼‰
- æ’é˜Ÿä»»åŠ¡: æ•°ç™¾ä¸ª

---

### å¤šæœºéƒ¨ç½²ï¼ˆæ‰©å±•æ–¹æ¡ˆï¼‰

#### æ¶æ„
```
                    è´Ÿè½½å‡è¡¡å™¨ (Nginx)
                         |
        +----------------+----------------+
        |                |                |
    æœåŠ¡å™¨1           æœåŠ¡å™¨2           æœåŠ¡å™¨3
    (2 GPU)          (2 GPU)          (2 GPU)
        |                |                |
        +----------------+----------------+
                         |
                  å…±äº«ä»»åŠ¡é˜Ÿåˆ— (Redis)
```

#### é…ç½®
```python
# ä½¿ç”¨Redisä½œä¸ºä»»åŠ¡é˜Ÿåˆ—
REDIS_URL = "redis://redis-server:6379"

# æ¯ä¸ªæœåŠ¡å™¨ç‹¬ç«‹ç®¡ç†æœ¬åœ°GPU
GPU_IDS = [0, 1]  # æœ¬æœºGPU
```

#### é¢„æœŸå¹¶å‘èƒ½åŠ›
- APIè¯·æ±‚: 3000+/ç§’
- åŒæ—¶è¿è¡Œä»»åŠ¡: 24ä¸ªï¼ˆ3æœåŠ¡å™¨ Ã— 8ä»»åŠ¡ï¼‰
- æ°´å¹³æ‰©å±•: å¯æ·»åŠ æ›´å¤šæœåŠ¡å™¨

---

## å¹¶å‘æµ‹è¯•è„šæœ¬

### å®Œæ•´æµ‹è¯•è„šæœ¬

åˆ›å»º `test_concurrency.py`:

```python
"""
å¹¶å‘æ€§èƒ½æµ‹è¯•è„šæœ¬
"""
import asyncio
import aiohttp
import time
from typing import List

BASE_URL = "http://localhost:8000"

async def create_training_task(session: aiohttp.ClientSession, index: int):
    """åˆ›å»ºè®­ç»ƒä»»åŠ¡"""
    url = f"{BASE_URL}/api/v2/training/start"
    data = {
        "model": "resnet18",
        "num_classes": 37,
        "train_path": "data/train",
        "val_path": "data/val",
        "save_path": f"models/test_{index}",
        "device": "cuda",
        "batch_size": 8,
        "num_epochs": 1
    }
    
    start = time.time()
    try:
        async with session.post(url, json=data) as resp:
            result = await resp.json()
            elapsed = time.time() - start
            return {
                "success": True,
                "task_id": result.get("task_id"),
                "time": elapsed
            }
    except Exception as e:
        elapsed = time.time() - start
        return {
            "success": False,
            "error": str(e),
            "time": elapsed
        }

async def query_task(session: aiohttp.ClientSession, task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    url = f"{BASE_URL}/api/v2/tasks/{task_id}"
    
    start = time.time()
    try:
        async with session.get(url) as resp:
            result = await resp.json()
            elapsed = time.time() - start
            return {
                "success": True,
                "status": result.get("status"),
                "time": elapsed
            }
    except Exception as e:
        elapsed = time.time() - start
        return {
            "success": False,
            "error": str(e),
            "time": elapsed
        }

async def test_concurrent_create(num_requests: int = 20):
    """æµ‹è¯•å¹¶å‘åˆ›å»ºä»»åŠ¡"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•1: å¹¶å‘åˆ›å»º {num_requests} ä¸ªè®­ç»ƒä»»åŠ¡")
    print(f"{'='*60}")
    
    async with aiohttp.ClientSession() as session:
        start = time.time()
        tasks = [create_training_task(session, i) for i in range(num_requests)]
        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start
        
        success = sum(1 for r in results if r["success"])
        avg_time = sum(r["time"] for r in results) / len(results)
        
        print(f"âœ… æˆåŠŸ: {success}/{num_requests}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time*1000:.2f}ms")
        print(f"ğŸ“Š QPS: {num_requests/elapsed:.2f}")
        
        return [r["task_id"] for r in results if r["success"]]

async def test_concurrent_query(task_ids: List[str], num_queries: int = 100):
    """æµ‹è¯•å¹¶å‘æŸ¥è¯¢"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•2: å¹¶å‘æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ {num_queries} æ¬¡")
    print(f"{'='*60}")
    
    if not task_ids:
        print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ID")
        return
    
    async with aiohttp.ClientSession() as session:
        start = time.time()
        # éšæœºæŸ¥è¯¢ä¸åŒçš„ä»»åŠ¡
        tasks = [
            query_task(session, task_ids[i % len(task_ids)])
            for i in range(num_queries)
        ]
        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start
        
        success = sum(1 for r in results if r["success"])
        avg_time = sum(r["time"] for r in results) / len(results)
        
        print(f"âœ… æˆåŠŸ: {success}/{num_queries}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time*1000:.2f}ms")
        print(f"ğŸ“Š QPS: {num_queries/elapsed:.2f}")

async def test_mixed_workload():
    """æµ‹è¯•æ··åˆå·¥ä½œè´Ÿè½½"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•3: æ··åˆå·¥ä½œè´Ÿè½½ï¼ˆåˆ›å»º+æŸ¥è¯¢ï¼‰")
    print(f"{'='*60}")
    
    async with aiohttp.ClientSession() as session:
        # 10ä¸ªåˆ›å»ºè¯·æ±‚
        create_tasks = [create_training_task(session, i) for i in range(10)]
        
        # å…ˆç­‰å¾…åˆ›å»ºå®Œæˆ
        create_results = await asyncio.gather(*create_tasks)
        task_ids = [r["task_id"] for r in create_results if r["success"]]
        
        # ç„¶å100ä¸ªæŸ¥è¯¢è¯·æ±‚
        query_tasks = [
            query_task(session, task_ids[i % len(task_ids)])
            for i in range(100)
        ]
        
        start = time.time()
        query_results = await asyncio.gather(*query_tasks)
        elapsed = time.time() - start
        
        print(f"âœ… åˆ›å»ºæˆåŠŸ: {len(task_ids)}/10")
        print(f"âœ… æŸ¥è¯¢æˆåŠŸ: {sum(1 for r in query_results if r['success'])}/100")
        print(f"â±ï¸  æŸ¥è¯¢æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"ğŸ“Š æŸ¥è¯¢QPS: {100/elapsed:.2f}")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("RFUAV Model Service - å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•1: å¹¶å‘åˆ›å»º
    task_ids = await test_concurrent_create(num_requests=20)
    
    await asyncio.sleep(2)  # ç­‰å¾…ä»»åŠ¡çŠ¶æ€æ›´æ–°
    
    # æµ‹è¯•2: å¹¶å‘æŸ¥è¯¢
    await test_concurrent_query(task_ids, num_queries=100)
    
    await asyncio.sleep(1)
    
    # æµ‹è¯•3: æ··åˆå·¥ä½œè´Ÿè½½
    await test_mixed_workload()
    
    print(f"\n{'='*60}")
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    asyncio.run(main())
```

### è¿è¡Œæµ‹è¯•
```bash
# å®‰è£…ä¾èµ–
pip install aiohttp

# ç¡®ä¿æœåŠ¡å·²å¯åŠ¨
python app_refactored.py

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæµ‹è¯•
python test_concurrency.py
```

### é¢„æœŸè¾“å‡º
```
============================================================
RFUAV Model Service - å¹¶å‘æ€§èƒ½æµ‹è¯•
============================================================

============================================================
æµ‹è¯•1: å¹¶å‘åˆ›å»º 20 ä¸ªè®­ç»ƒä»»åŠ¡
============================================================
âœ… æˆåŠŸ: 20/20
â±ï¸  æ€»è€—æ—¶: 0.85ç§’
â±ï¸  å¹³å‡å“åº”æ—¶é—´: 42.50ms
ğŸ“Š QPS: 23.53

============================================================
æµ‹è¯•2: å¹¶å‘æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ 100 æ¬¡
============================================================
âœ… æˆåŠŸ: 100/100
â±ï¸  æ€»è€—æ—¶: 0.32ç§’
â±ï¸  å¹³å‡å“åº”æ—¶é—´: 3.20ms
ğŸ“Š QPS: 312.50

============================================================
âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼
============================================================
```

---

## æ€»ç»“

### âœ… å½“å‰æ¶æ„çš„å¹¶å‘ä¼˜åŠ¿

1. **APIå±‚å®Œå…¨éé˜»å¡** - ä½¿ç”¨async/await
2. **åå°ä»»åŠ¡æ‰§è¡Œ** - ä½¿ç”¨BackgroundTasks
3. **èµ„æºç®¡ç†çº¿ç¨‹å®‰å…¨** - ä½¿ç”¨threading.Lock
4. **ä»»åŠ¡æ’é˜Ÿæœºåˆ¶** - é˜²æ­¢èµ„æºè¿‡è½½
5. **GPUæ™ºèƒ½è°ƒåº¦** - è‡ªåŠ¨è´Ÿè½½å‡è¡¡

### âš ï¸ éœ€è¦æ³¨æ„çš„ç‚¹

1. **çº¿ç¨‹æ± å¤§å°** - å¤§é‡æ’é˜Ÿä»»åŠ¡å¯èƒ½è€—å°½çº¿ç¨‹
2. **GPUæ˜¾å­˜** - éœ€è¦æ ¹æ®æ˜¾å­˜è°ƒæ•´å¹¶å‘æ•°
3. **ä»»åŠ¡æŒä¹…åŒ–** - å½“å‰ä½¿ç”¨å†…å­˜ï¼Œé‡å¯åä¸¢å¤±

### ğŸš€ æ¨èé…ç½®

#### ç”Ÿäº§ç¯å¢ƒ
```bash
# Uvicorné…ç½®
uvicorn app_refactored:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --limit-concurrency 1000 \
    --backlog 2048

# GPUå¹¶å‘é™åˆ¶ï¼ˆ.envï¼‰
MAX_TRAINING_CONCURRENT_GPU=1
MAX_INFERENCE_CONCURRENT_GPU=3
```

#### é¢„æœŸæ€§èƒ½
- âœ… APIè¯·æ±‚: **1000+ QPS**
- âœ… ä»»åŠ¡åˆ›å»ºå“åº”: **< 50ms**
- âœ… çŠ¶æ€æŸ¥è¯¢å“åº”: **< 20ms**
- âœ… åŒæ—¶è¿è¡Œä»»åŠ¡: **8ä¸ª** (2 GPU)
- âœ… æ’é˜Ÿä»»åŠ¡: **æ•°ç™¾ä¸ª**

### ğŸ“ ç›‘æ§å»ºè®®

1. **ç›‘æ§APIå“åº”æ—¶é—´**
2. **ç›‘æ§GPUåˆ©ç”¨ç‡**
3. **ç›‘æ§ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦**
4. **ç›‘æ§çº¿ç¨‹æ± ä½¿ç”¨ç‡**
5. **ç›‘æ§å†…å­˜ä½¿ç”¨**

---

**ç»“è®º**: å½“å‰æ¶æ„è®¾è®¡åˆç†ï¼Œ**ä¸ä¼šå‡ºç°é˜»å¡é—®é¢˜**ã€‚APIæ¥å£å“åº”è¿…é€Ÿï¼Œä»»åŠ¡åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œèµ„æºç®¡ç†çº¿ç¨‹å®‰å…¨ã€‚åœ¨æ¨èé…ç½®ä¸‹ï¼Œå¯ä»¥è½»æ¾å¤„ç†é«˜å¹¶å‘åœºæ™¯ã€‚

---

**ç‰ˆæœ¬**: V2.3.1  
**åˆ›å»ºæ—¥æœŸ**: 2024-01  
**æœ€åæ›´æ–°**: 2024-01


