# å¹¶å‘ç‰ˆæœ¬å¿«é€Ÿå¼€å§‹

## 30ç§’å¿«é€Ÿä½“éªŒ

```bash
# 1. å¯åŠ¨æœåŠ¡
python app_concurrent.py

# 2. è¿è¡Œç¤ºä¾‹ï¼ˆæ–°ç»ˆç«¯ï¼‰
python concurrent_example.py

# 3. é€‰æ‹©åœºæ™¯1
```

## 5åˆ†é’Ÿå®Œæ•´æ¼”ç¤º

### æ­¥éª¤1: å¯åŠ¨æœåŠ¡ (1åˆ†é’Ÿ)

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements_enhanced.txt

# å¯åŠ¨å¹¶å‘ä¼˜åŒ–ç‰ˆæœåŠ¡
python app_concurrent.py
```

çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºè¡¨ç¤ºæˆåŠŸï¼š
```
INFO: Application startup complete.
INFO: Uvicorn running on http://0.0.0.0:8000
```

### æ­¥éª¤2: æµ‹è¯•åŸºç¡€åŠŸèƒ½ (2åˆ†é’Ÿ)

æ‰“å¼€æ–°ç»ˆç«¯ï¼Œè¿è¡Œï¼š

```python
import requests

# æ£€æŸ¥æœåŠ¡
response = requests.get("http://localhost:8000")
print(response.json())

# æŸ¥çœ‹èµ„æºçŠ¶æ€
response = requests.get("http://localhost:8000/api/v2/resources")
print(response.json())
```

### æ­¥éª¤3: å¯åŠ¨å¹¶å‘ä»»åŠ¡ (2åˆ†é’Ÿ)

```python
import requests

API_BASE = "http://localhost:8000"

# å¯åŠ¨è®­ç»ƒä»»åŠ¡
train_config = {
    "model": "resnet18",
    "num_classes": 37,
    "train_path": "data/train",
    "val_path": "data/val",
    "batch_size": 32,
    "num_epochs": 10,
    "device": "cuda",
    "save_path": "models/test",
    "priority": 5
}

train_response = requests.post(f"{API_BASE}/api/v2/train", json=train_config)
train_id = train_response.json()["task_id"]
print(f"è®­ç»ƒä»»åŠ¡ID: {train_id}")

# å¯åŠ¨æ¨ç†ä»»åŠ¡ï¼ˆä¸è®­ç»ƒåŒæ—¶è¿›è¡Œï¼‰
infer_config = {
    "cfg_path": "configs/exp3.1_ResNet18.yaml",
    "weight_path": "models/best_model.pth",
    "source_path": "example/test_data/",
    "device": "cuda",
    "priority": 3  # ä¼˜å…ˆçº§é«˜äºè®­ç»ƒ
}

infer_response = requests.post(f"{API_BASE}/api/v2/inference", json=infer_config)
infer_id = infer_response.json()["task_id"]
print(f"æ¨ç†ä»»åŠ¡ID: {infer_id}")

# æŸ¥çœ‹èµ„æºä½¿ç”¨
response = requests.get(f"{API_BASE}/api/v2/resources")
status = response.json()
print(f"\nGPUè®­ç»ƒä»»åŠ¡: {status['device_usage']['cuda']['training']}")
print(f"GPUæ¨ç†ä»»åŠ¡: {status['device_usage']['cuda']['inference']}")
```

## å¸¸ç”¨åœºæ™¯

### åœºæ™¯A: ç”Ÿäº§ç¯å¢ƒï¼ˆæ¨èé…ç½®ï¼‰

```python
# GPUä¸“ç”¨äºæ¨ç†ï¼ŒCPUç”¨äºè®­ç»ƒ
train_config = {
    "model": "mobilenet_v3_small",  # è½»é‡çº§æ¨¡å‹
    "device": "cpu",                # CPUè®­ç»ƒ
    "batch_size": 8,
    "priority": 7,                  # ä½ä¼˜å…ˆçº§
    # ... å…¶ä»–å‚æ•°
}

infer_config = {
    "device": "cuda",               # GPUæ¨ç†
    "priority": 2,                  # é«˜ä¼˜å…ˆçº§
    # ... å…¶ä»–å‚æ•°
}
```

### åœºæ™¯B: æ‰¹é‡æ¨ç†

```python
# åŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®é›†
datasets = [
    "data/dataset1/",
    "data/dataset2/",
    "data/dataset3/"
]

for dataset in datasets:
    config = {
        "cfg_path": "configs/exp3.1_ResNet18.yaml",
        "weight_path": "models/best_model.pth",
        "source_path": dataset,
        "device": "cuda",
        "priority": 3
    }
    
    response = requests.post(f"{API_BASE}/api/v2/inference", json=config)
    print(f"æ¨ç†ä»»åŠ¡å·²å¯åŠ¨: {dataset}")

# ç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒåº¦ï¼Œæœ€å¤š3ä¸ªåŒæ—¶è¿è¡Œ
```

### åœºæ™¯C: å¼€å‘æµ‹è¯•

```python
# è®­ç»ƒæ–°æ¨¡å‹æ—¶ï¼Œæµ‹è¯•æ—§æ¨¡å‹
train_config = {
    "model": "resnet50",
    "device": "cuda",
    "priority": 6,  # ä¸­ç­‰ä¼˜å…ˆçº§
    # ... å…¶ä»–å‚æ•°
}

test_infer_config = {
    "device": "cuda",
    "priority": 4,  # ç•¥é«˜ä¼˜å…ˆçº§
    # ... å…¶ä»–å‚æ•°
}
```

## ç›‘æ§å‘½ä»¤

### å®æ—¶ç›‘æ§èµ„æº

```python
import requests
import time

while True:
    response = requests.get("http://localhost:8000/api/v2/resources")
    status = response.json()
    
    print(f"\rè®­ç»ƒ: {status['device_usage']['cuda']['training']}, "
          f"æ¨ç†: {status['device_usage']['cuda']['inference']}", end="")
    
    time.sleep(2)
```

### æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡

```python
response = requests.get("http://localhost:8000/api/v2/tasks")
tasks = response.json()

print(f"è®­ç»ƒä»»åŠ¡: {tasks['total_training']}")
print(f"æ¨ç†ä»»åŠ¡: {tasks['total_inference']}")

for task in tasks['training_tasks']:
    print(f"  {task['task_id'][:8]}: {task['status']}")
```

### æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—

```python
response = requests.get(
    f"http://localhost:8000/api/v2/tasks/{task_id}/logs",
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))
```

## é…ç½®è°ƒä¼˜

### å¢åŠ æ¨ç†å¹¶å‘æ•°

```python
# å¦‚æœGPUæ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å¢åŠ å¹¶å‘æ•°
config = {
    "max_concurrent": {
        "cuda": {
            "training": 1,
            "inference": 5  # ä»é»˜è®¤3å¢åŠ åˆ°5
        }
    }
}

requests.post(
    "http://localhost:8000/api/v2/resources/config",
    json=config
)
```

### ä¼˜å…ˆçº§è®¾ç½®å»ºè®®

```python
# ç”Ÿäº§ç¯å¢ƒ
å®æ—¶æ¨ç† = 1-2    # æœ€é«˜
æ‰¹é‡æ¨ç† = 3-5    # ä¸­é«˜
åå°è®­ç»ƒ = 7-9    # æœ€ä½

# å¼€å‘ç¯å¢ƒ
å¿«é€Ÿå®éªŒ = 2-3
æ­£å¸¸è®­ç»ƒ = 4-6
æµ‹è¯•ä»»åŠ¡ = 7-9
```

## å¸¸è§é—®é¢˜

### Q: ä»»åŠ¡ä¸€ç›´åœ¨æ’é˜Ÿï¼Ÿ
**A**: èµ„æºå·²æ»¡ï¼Œç­‰å¾…å…¶ä»–ä»»åŠ¡å®Œæˆã€‚æŸ¥çœ‹èµ„æºçŠ¶æ€ï¼š
```python
requests.get("http://localhost:8000/api/v2/resources")
```

### Q: GPUæ˜¾å­˜ä¸è¶³ï¼Ÿ
**A**: å‡å°batch_sizeæˆ–ä½¿ç”¨CPUï¼š
```python
config = {
    "batch_size": 8,  # ä»32é™åˆ°8
    # æˆ–
    "device": "cpu"
}
```

### Q: æ¨ç†è¢«è®­ç»ƒé˜»å¡ï¼Ÿ
**A**: æé«˜æ¨ç†ä¼˜å…ˆçº§ï¼š
```python
infer_config = {"priority": 1}  # æœ€é«˜ä¼˜å…ˆçº§
train_config = {"priority": 9}  # æœ€ä½ä¼˜å…ˆçº§
```

## å®Œæ•´ç¤ºä¾‹è„šæœ¬

### ç¤ºä¾‹1: ç®€å•å¹¶å‘

```python
#!/usr/bin/env python3
"""ç®€å•å¹¶å‘ç¤ºä¾‹"""
import requests
import time

API_BASE = "http://localhost:8000"

def main():
    # å¯åŠ¨è®­ç»ƒ
    print("å¯åŠ¨è®­ç»ƒ...")
    train_config = {
        "model": "resnet18",
        "num_classes": 37,
        "train_path": "data/train",
        "val_path": "data/val",
        "device": "cuda",
        "save_path": "models/test",
        "priority": 5
    }
    
    response = requests.post(f"{API_BASE}/api/v2/train", json=train_config)
    train_id = response.json()["task_id"]
    print(f"è®­ç»ƒID: {train_id[:8]}")
    
    # ç­‰å¾…2ç§’
    time.sleep(2)
    
    # å¯åŠ¨æ¨ç†
    print("å¯åŠ¨æ¨ç†...")
    infer_config = {
        "cfg_path": "configs/exp3.1_ResNet18.yaml",
        "weight_path": "models/best_model.pth",
        "source_path": "example/test_data/",
        "device": "cuda",
        "priority": 3
    }
    
    response = requests.post(f"{API_BASE}/api/v2/inference", json=infer_config)
    infer_id = response.json()["task_id"]
    print(f"æ¨ç†ID: {infer_id[:8]}")
    
    # ç›‘æ§èµ„æº
    print("\nç›‘æ§ä¸­...")
    for _ in range(10):
        response = requests.get(f"{API_BASE}/api/v2/resources")
        status = response.json()
        print(f"è®­ç»ƒ: {status['device_usage']['cuda']['training']}, "
              f"æ¨ç†: {status['device_usage']['cuda']['inference']}")
        time.sleep(3)

if __name__ == "__main__":
    main()
```

### ç¤ºä¾‹2: ä½¿ç”¨é¢„è®¾ç¤ºä¾‹

```bash
# æœ€ç®€å•çš„æ–¹å¼
python concurrent_example.py

# æŒ‰æç¤ºé€‰æ‹©åœºæ™¯
```

## ä¸‹ä¸€æ­¥

1. âœ… è¿è¡Œ `concurrent_example.py` ä½“éªŒ5ä¸ªåœºæ™¯
2. ğŸ“– é˜…è¯» `CONCURRENT_USAGE.md` äº†è§£è¯¦ç»†ç”¨æ³•
3. ğŸ“Š é˜…è¯» `VERSION_COMPARISON.md` äº†è§£ç‰ˆæœ¬å·®å¼‚
4. ğŸ”§ æ ¹æ®éœ€æ±‚è°ƒæ•´èµ„æºé…ç½®

## æ–‡æ¡£é“¾æ¥

- [å¹¶å‘ä½¿ç”¨æŒ‡å—](CONCURRENT_USAGE.md)
- [ç‰ˆæœ¬å¯¹æ¯”](VERSION_COMPARISON.md)
- [å®Œæ•´æ€»ç»“](SUMMARY.md)
- [APIæ–‡æ¡£](http://localhost:8000/docs)

## æŠ€æœ¯æ”¯æŒ

- ç¤ºä¾‹ä»£ç : `concurrent_example.py`
- æºç : `app_concurrent.py`
- åœ¨çº¿æ–‡æ¡£: http://localhost:8000/docs

