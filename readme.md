# RFUAV Model Service - å®Œæ•´æ–‡æ¡£

> ğŸš€ æ— äººæœºä¿¡å·è¯†åˆ«æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æœåŠ¡ - é‡æ„ç‰ˆ V2.4.0

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

## ğŸ“‘ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
- [é¡¹ç›®æ¶æ„](#é¡¹ç›®æ¶æ„)
- [GPUè®¾å¤‡é€‰æ‹©](#gpuè®¾å¤‡é€‰æ‹©)
- [APIæ¥å£](#apiæ¥å£)
- [JSONæ ¼å¼è§„èŒƒ](#jsonæ ¼å¼è§„èŒƒ)
- [Webæµ‹è¯•ç•Œé¢](#webæµ‹è¯•ç•Œé¢)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
- [ç‰ˆæœ¬å†å²](#ç‰ˆæœ¬å†å²)

---

## é¡¹ç›®æ¦‚è¿°

### ç®€ä»‹

RFUAV Model Service æ˜¯ä¸€ä¸ªåŸºäºFastAPIçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œä¸“ä¸ºæ— äººæœºä¿¡å·è¯†åˆ«è®¾è®¡ã€‚ç³»ç»Ÿé‡‡ç”¨æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œæ”¯æŒçµæ´»çš„GPUè®¾å¤‡é€‰æ‹©å’Œæ™ºèƒ½èµ„æºç®¡ç†ã€‚

### æ ¸å¿ƒç‰¹æ€§

#### ğŸ¯ åŠŸèƒ½ç‰¹æ€§
- âœ… **å‚æ•°åŒ–è®­ç»ƒ** - æ— éœ€é…ç½®æ–‡ä»¶ï¼ŒAPIç›´æ¥æŒ‡å®šæ‰€æœ‰è®­ç»ƒå‚æ•°
- âœ… **è¯¦ç»†è®­ç»ƒæŒ‡æ ‡** - å®æ—¶è¿”å›lossã€accuracyã€F1ã€mAPç­‰15+ç§æŒ‡æ ‡
- âœ… **æ•°æ®é¢„å¤„ç†** - æ•°æ®é›†åˆ†å‰²ã€æ•°æ®å¢å¼ºã€å›¾åƒè£å‰ªä¸€ç«™å¼è§£å†³æ–¹æ¡ˆ â­æ–°
- âœ… **çµæ´»æ¨ç†** - æ”¯æŒå•æ¬¡æ¨ç†å’Œæ‰¹é‡æ¨ç†
- âœ… **å®æ—¶æ—¥å¿—æµ** - Server-Sent Eventså®æ—¶æµå¼ä¼ è¾“è®­ç»ƒæ—¥å¿—å’ŒæŒ‡æ ‡
- âœ… **æ™ºèƒ½GPUè°ƒåº¦** - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜GPUæˆ–æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡
- âœ… **å¤šGPUæ”¯æŒ** - æ”¯æŒcuda:0ã€cuda:1ç­‰å¤šGPUé€‰æ‹©
- âœ… **å¹¶å‘ä¼˜åŒ–** - æ™ºèƒ½èµ„æºç®¡ç†å’Œä»»åŠ¡é˜Ÿåˆ—è°ƒåº¦
- âœ… **ä»»åŠ¡ç®¡ç†** - å®Œæ•´çš„ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†
- âœ… **æ ‡å‡†JSON** - æ‰€æœ‰è¯·æ±‚å’Œå“åº”éƒ½æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼

#### ğŸ—ï¸ æ¶æ„ç‰¹æ€§
- ğŸ“‚ **æ¸…æ™°åˆ†å±‚** - è·¯ç”±å±‚ã€æœåŠ¡å±‚ã€æ¨¡å‹å±‚ã€æ ¸å¿ƒå±‚åˆ†ç¦»
- ğŸ”„ **é«˜å†…èšä½è€¦åˆ** - æ¨¡å—èŒè´£å•ä¸€ï¼Œæ˜“äºç»´æŠ¤
- ğŸ§ª **å¯æµ‹è¯•æ€§å¼º** - æ¯å±‚å¯ç‹¬ç«‹æµ‹è¯•
- ğŸ“ˆ **æ˜“äºæ‰©å±•** - æ·»åŠ æ–°åŠŸèƒ½åªéœ€3æ­¥
- ğŸ“– **ä»£ç æ˜“è¯»** - ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šå®Œæ•´

#### ğŸ® GPUç‰¹æ€§
- âœ… **å¯åŠ¨æ˜¾ç¤ºGPUä¿¡æ¯** - æœåŠ¡å¯åŠ¨æ—¶è‡ªåŠ¨è¾“å‡ºæ‰€æœ‰GPUè¯¦ç»†ä¿¡æ¯
- âœ… **å®æ—¶ç›‘æ§** - æ˜¾å­˜ä½¿ç”¨ã€åˆ©ç”¨ç‡ã€ä»»åŠ¡æ•°é‡å®æ—¶è¿½è¸ª
- âœ… **æ™ºèƒ½é€‰æ‹©** - ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU
- âœ… **æ‰‹åŠ¨æŒ‡å®š** - æ”¯æŒæŒ‡å®šå…·ä½“GPUè®¾å¤‡ï¼ˆcuda:0ã€cuda:1ç­‰ï¼‰
- âœ… **è´Ÿè½½å‡è¡¡** - å¤šGPUç¯å¢ƒä¸‹è‡ªåŠ¨è´Ÿè½½å‡è¡¡

### æ”¯æŒçš„æ¨¡å‹

- **ResNetç³»åˆ—**: resnet18, resnet34, resnet50, resnet101, resnet152
- **ViTç³»åˆ—**: vit_b_16, vit_b_32, vit_l_16, vit_l_32
- **Swin Transformer**: swin_v2_t, swin_v2_s, swin_v2_b
- **MobileNet**: mobilenet_v3_large, mobilenet_v3_small

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- CUDA 11.0+ (å¦‚æœä½¿ç”¨GPU)
- PyTorch 2.0+

#### å®‰è£…ä¾èµ–

**åŸºç¡€å®‰è£…**ï¼ˆæ¨èï¼‰ï¼š
```bash
# GPUç‰ˆæœ¬ï¼ˆCUDA 11.8ï¼‰
pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

**å…¶ä»–å®‰è£…æ–¹å¼**ï¼š
```bash
# CPUç‰ˆæœ¬
pip install torch==2.0.1+cpu torchvision==0.15.2+cpu --extra-index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt

# å¼€å‘ç¯å¢ƒï¼ˆåŒ…å«æµ‹è¯•å’Œä»£ç æ£€æŸ¥å·¥å…·ï¼‰
pip install -r requirements-dev.txt

# æµ‹è¯•ç¯å¢ƒ
pip install -r requirements-test.txt

# ç”Ÿäº§ç¯å¢ƒï¼ˆåŒ…å«ç›‘æ§å’Œä¼˜åŒ–å·¥å…·ï¼‰
pip install -r requirements-prod.txt
```

**éªŒè¯å®‰è£…**ï¼š
```bash
python check_installation.py
```

ä¸»è¦ä¾èµ–ï¼š
```
# Webæ¡†æ¶
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# æ•°æ®éªŒè¯
pydantic>=2.0.0
pydantic-settings>=2.0.0

# æ·±åº¦å­¦ä¹ 
torch>=2.0.0
torchvision>=0.15.0

# å›¾åƒå¤„ç†
opencv-python>=4.8.0
Pillow>=10.0.0
albumentations>=1.4.0

# æ•°æ®å¤„ç†
numpy>=1.24.0
pandas>=2.0.0
```

å®Œæ•´ä¾èµ–åˆ—è¡¨å‚è§ [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md)

### 2. é…ç½®ç¯å¢ƒï¼ˆå¯é€‰ï¼‰

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶
cp env.example .env

# ç¼–è¾‘é…ç½®
nano .env
```

ä¸»è¦é…ç½®é¡¹ï¼š
```bash
# æœåŠ¡å™¨é…ç½®
HOST=0.0.0.0
PORT=8000

# èµ„æºé™åˆ¶
MAX_TRAINING_CONCURRENT_GPU=1
MAX_INFERENCE_CONCURRENT_GPU=3

# æ”¯æŒçš„æ¨¡å‹
SUPPORTED_MODELS="resnet18,resnet50,vit_b_16,..."
```

### 3. å¯åŠ¨æœåŠ¡

> **æ³¨æ„**: é¡¹ç›®åŒ…å«3ä¸ªç‰ˆæœ¬çš„appæ–‡ä»¶ï¼Œæ¨èä½¿ç”¨ `app_refactored.py` (V2.3.1)
> 
> è¯¦ç»†ç‰ˆæœ¬è¯´æ˜è¯·å‚è€ƒï¼š[APP_VERSIONS_GUIDE.md](APP_VERSIONS_GUIDE.md)

#### æ–¹å¼1: Pythonç›´æ¥è¿è¡Œï¼ˆæ¨èï¼‰
```bash
python app_refactored.py
```

#### æ–¹å¼2: Uvicorn
```bash
uvicorn app_refactored:app --host 0.0.0.0 --port 8000 --reload
```

#### æ–¹å¼3: ä½¿ç”¨å¯åŠ¨è„šæœ¬
```bash
# Windows
start_refactored.bat

# Linux/Mac
./start_refactored.sh
```

#### å…¶ä»–ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
```bash
# V2.2.0 - å¹¶å‘ä¼˜åŒ–ç‰ˆ
python app_concurrent.py

# V2.0.0 - å¢å¼ºç‰ˆ
python app_enhanced.py
```

### 4. éªŒè¯æœåŠ¡

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health

# æŸ¥çœ‹GPUä¿¡æ¯
curl http://localhost:8000/api/v2/resources/gpu

# æŸ¥çœ‹APIæ–‡æ¡£
# æµè§ˆå™¨è®¿é—®: http://localhost:8000/docs
```

### 5. å¯åŠ¨æ—¶GPUä¿¡æ¯æ˜¾ç¤º

æœåŠ¡å¯åŠ¨åä¼šè‡ªåŠ¨æ˜¾ç¤ºï¼š
```
======================================================================
ğŸš€ GPUç¡¬ä»¶ä¿¡æ¯
======================================================================
âœ… GPUå¯ç”¨
ğŸ“¦ CUDAç‰ˆæœ¬: 11.8
ğŸ”§ PyTorchç‰ˆæœ¬: 2.0.1
ğŸ¯ æ£€æµ‹åˆ° 2 ä¸ªGPUè®¾å¤‡:

  GPU 0 (cuda:0)
  â”œâ”€ å‹å·: NVIDIA GeForce RTX 3090
  â”œâ”€ Compute Capability: 8.6
  â”œâ”€ æ€»æ˜¾å­˜: 24.00 GB
  â”œâ”€ å·²ç”¨æ˜¾å­˜: 0.50 GB (2.1%)
  â”œâ”€ ç©ºé—²æ˜¾å­˜: 23.50 GB
  â””â”€ å½“å‰ä»»åŠ¡: è®­ç»ƒ=0, æ¨ç†=0
======================================================================
```

---

## æ ¸å¿ƒåŠŸèƒ½

### 0. æ•°æ®é¢„å¤„ç† â­æ–°

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæ‚¨å¯èƒ½éœ€è¦å‡†å¤‡æ•°æ®é›†ã€‚ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•°æ®é¢„å¤„ç†åŠŸèƒ½ã€‚

#### æ•°æ®é›†åˆ†å‰²
```python
from test_refactored_api import RFUAVClient

client = RFUAVClient("http://localhost:8000")

# åˆ†å‰²æ•°æ®é›†ä¸ºtrain/val/test
result = client.split_dataset(
    input_path="data/raw_dataset",
    output_path="data/split_dataset",
    train_ratio=0.7,   # 70% è®­ç»ƒé›†
    val_ratio=0.2      # 20% éªŒè¯é›†ï¼Œå‰©ä½™10%æµ‹è¯•é›†
)
task_id = result['task_id']
print(f"åˆ†å‰²ä»»åŠ¡å·²å¯åŠ¨: {task_id}")
```

#### æ•°æ®å¢å¼º
```python
# å¯¹æ•°æ®é›†è¿›è¡Œå¢å¼º
result = client.augment_dataset(
    dataset_path="data/split_dataset",
    output_path="data/augmented_dataset",
    methods=["CLAHE", "ColorJitter", "GaussNoise"]  # é€‰æ‹©å¢å¼ºæ–¹æ³•
)
task_id = result['task_id']
print(f"å¢å¼ºä»»åŠ¡å·²å¯åŠ¨: {task_id}")
```

#### å›¾åƒè£å‰ª
```python
# æ‰¹é‡è£å‰ªå›¾åƒ
result = client.crop_images(
    input_path="data/images",
    output_path="data/cropped",
    x=100, y=100,
    width=500, height=500
)
task_id = result['task_id']
print(f"è£å‰ªä»»åŠ¡å·²å¯åŠ¨: {task_id}")
```

**æ”¯æŒçš„æ•°æ®å¢å¼ºæ–¹æ³•**:
- **AdvancedBlur** - é«˜çº§æ¨¡ç³Š
- **CLAHE** - å¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
- **ColorJitter** - é¢œè‰²æŠ–åŠ¨ï¼ˆäº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦ï¼‰
- **GaussNoise** - é«˜æ–¯å™ªå£°
- **ISONoise** - ISOå™ªå£°
- **Sharpen** - é”åŒ–

è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼š[æ•°æ®é¢„å¤„ç†å®Œæ•´æ–‡æ¡£](./PREPROCESSING_GUIDE.md)

---

### 1. è®­ç»ƒä»»åŠ¡

#### å¯åŠ¨è®­ç»ƒ
```python
from test_refactored_api import RFUAVClient

client = RFUAVClient("http://localhost:8000")

# è‡ªåŠ¨é€‰æ‹©GPU
result = client.start_training(
    model="resnet18",
    num_classes=37,
    train_path="data/train",
    val_path="data/val",
    save_path="models/output",
    batch_size=16,
    num_epochs=50,
    device="cuda"  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜GPU
)

print(f"Task ID: {result['task_id']}")
print(f"Device: {result['device']}")
```

#### æŒ‡å®šGPUè®­ç»ƒ
```python
# ä½¿ç”¨GPU 0
result = client.start_training(
    model="resnet50",
    num_classes=37,
    train_path="data/train",
    val_path="data/val",
    save_path="models/output_gpu0",
    device="cuda:0"  # æŒ‡å®šGPU 0
)
```

#### æŸ¥çœ‹è®­ç»ƒçŠ¶æ€ï¼ˆå«è¯¦ç»†æŒ‡æ ‡ï¼‰â­æ–°
```python
# æŸ¥è¯¢çŠ¶æ€
status = client.get_task(task_id)
print(f"Status: {status['status']}")
print(f"Progress: {status['progress']}%")

# æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæŒ‡æ ‡
if status.get('latest_metrics'):
    metrics = status['latest_metrics']
    print(f"\nå½“å‰è®­ç»ƒæŒ‡æ ‡:")
    print(f"  Epoch: {status['current_epoch']}/{status['total_epochs']}")
    print(f"  è®­ç»ƒæŸå¤±: {metrics.get('train_loss', 'N/A')}")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {metrics.get('train_acc', 'N/A')}%")
    print(f"  éªŒè¯æŸå¤±: {metrics.get('val_loss', 'N/A')}")
    print(f"  éªŒè¯å‡†ç¡®ç‡: {metrics.get('val_acc', 'N/A')}%")
    print(f"  Macro F1: {metrics.get('macro_f1', 'N/A')}")
    print(f"  mAP: {metrics.get('mAP', 'N/A')}")
    print(f"  Top-1å‡†ç¡®ç‡: {metrics.get('top1_acc', 'N/A')}%")
    print(f"  æœ€ä½³å‡†ç¡®ç‡: {metrics.get('best_acc', 'N/A')}%")
```

#### å®æ—¶æ—¥å¿—ï¼ˆå«æŒ‡æ ‡ï¼‰â­æ–°
```python
# è¿æ¥æ—¥å¿—æµï¼ˆåŒ…å«è®­ç»ƒæŒ‡æ ‡ï¼‰
for log in client.stream_training_logs(task_id):
    print(f"[{log['level']}] {log['message']}")
    
    # å¦‚æœæ—¥å¿—åŒ…å«è®­ç»ƒæŒ‡æ ‡ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if log.get('metrics'):
        metrics = log['metrics']
        stage = log.get('stage', '')
        print(f"  â””â”€ é˜¶æ®µ: {stage}")
        if metrics.get('train_acc'):
            print(f"  â””â”€ è®­ç»ƒå‡†ç¡®ç‡: {metrics['train_acc']}%")
        if metrics.get('val_acc'):
            print(f"  â””â”€ éªŒè¯å‡†ç¡®ç‡: {metrics['val_acc']}%")
```

### 2. æ¨ç†ä»»åŠ¡

#### å•æ¬¡æ¨ç†
```python
result = client.start_inference(
    cfg_path="configs/model.yaml",
    weight_path="models/best.pth",
    source_path="data/test",
    device="cuda:1"  # ä½¿ç”¨GPU 1
)
```

#### æ‰¹é‡æ¨ç†
```python
result = client.start_batch_inference(
    cfg_path="configs/model.yaml",
    weight_path="models/best.pth",
    source_paths=[
        "data/test1",
        "data/test2",
        "data/test3"
    ],
    device="cuda"  # è‡ªåŠ¨åˆ†é…
)
print(f"Started {result['total']} tasks")
```

### 3. GPUè®¾å¤‡é€‰æ‹©

#### ä¸‰ç§é€‰æ‹©æ–¹å¼

**1. è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰**
```python
device = "cuda"  # ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU
```

**2. æŒ‡å®šGPU 0**
```python
device = "cuda:0"  # å›ºå®šä½¿ç”¨ç¬¬ä¸€å—GPU
```

**3. æŒ‡å®šGPU 1**
```python
device = "cuda:1"  # å›ºå®šä½¿ç”¨ç¬¬äºŒå—GPU
```

**4. ä½¿ç”¨CPU**
```python
device = "cpu"  # ä½¿ç”¨CPUè®¡ç®—
```

#### æŸ¥çœ‹GPUä¿¡æ¯
```python
gpu_info = client.get_gpu_info()
print(f"GPU Count: {gpu_info['count']}")
for device in gpu_info['devices']:
    print(f"GPU {device['id']}: {device['name']}")
    print(f"  Memory: {device['free_memory_gb']:.2f} GB free")
    print(f"  Tasks: {device['current_tasks']}")
```

### 4. èµ„æºç›‘æ§

```python
# æŸ¥çœ‹èµ„æºçŠ¶æ€
resources = client.get_resources()

# è®¾å¤‡ä½¿ç”¨æƒ…å†µ
for device, usage in resources['device_usage'].items():
    print(f"{device}: Train={usage['training']}, Infer={usage['inference']}")

# èµ„æºé™åˆ¶
for device, limits in resources['limits'].items():
    print(f"{device}: Max Train={limits['training']}, Max Infer={limits['inference']}")
```

### 5. ä»»åŠ¡ç®¡ç†

```python
# è·å–æ‰€æœ‰ä»»åŠ¡
tasks = client.get_all_tasks()
print(f"Training: {tasks['total_training']}")
print(f"Inference: {tasks['total_inference']}")

# å–æ¶ˆä»»åŠ¡
client.cancel_task(task_id)

# åˆ é™¤ä»»åŠ¡è®°å½•
client.delete_task(task_id)
```

---

## é¡¹ç›®æ¶æ„

### ç›®å½•ç»“æ„

```
RFUAV-server/
â”‚
â”œâ”€â”€ app_refactored.py              # ä¸»åº”ç”¨å…¥å£ â­
â”‚
â”œâ”€â”€ api/                           # APIå±‚ - è·¯ç”±å®šä¹‰
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ training.py            # è®­ç»ƒæ¥å£
â”‚       â”œâ”€â”€ inference.py           # æ¨ç†æ¥å£
â”‚       â”œâ”€â”€ tasks.py               # ä»»åŠ¡ç®¡ç†
â”‚       â”œâ”€â”€ resources.py           # èµ„æºç®¡ç†
â”‚       â””â”€â”€ health.py              # å¥åº·æ£€æŸ¥
â”‚
â”œâ”€â”€ services/                      # æœåŠ¡å±‚ - ä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ base_service.py            # åŸºç¡€æœåŠ¡
â”‚   â”œâ”€â”€ training_service.py        # è®­ç»ƒæœåŠ¡
â”‚   â”œâ”€â”€ inference_service.py       # æ¨ç†æœåŠ¡
â”‚   â””â”€â”€ task_service.py            # ä»»åŠ¡ç®¡ç†
â”‚
â”œâ”€â”€ models/                        # æ•°æ®æ¨¡å‹å±‚
â”‚   â””â”€â”€ schemas.py                 # Pydanticæ¨¡å‹
â”‚
â”œâ”€â”€ core/                          # æ ¸å¿ƒå±‚
â”‚   â”œâ”€â”€ config.py                  # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ resource_manager.py        # èµ„æºç®¡ç†å™¨
â”‚
â”œâ”€â”€ utils/                         # å·¥å…·å±‚ï¼ˆåŸæœ‰ï¼‰
â”‚   â”œâ”€â”€ trainer.py                 # è®­ç»ƒå™¨
â”‚   â””â”€â”€ benchmark.py               # æ¨ç†æµ‹è¯•
â”‚
â”œâ”€â”€ test_web_ui.html              # Webæµ‹è¯•ç•Œé¢
â”œâ”€â”€ test_refactored_api.py        # Pythonæµ‹è¯•å®¢æˆ·ç«¯
â””â”€â”€ test_json_format.py           # JSONæ ¼å¼æµ‹è¯•
```

### æ¶æ„å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å®¢æˆ·ç«¯è¯·æ±‚ (JSON)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    è·¯ç”±å±‚ (API Routers)                      â”‚
â”‚    - æ¥æ”¶è¯·æ±‚                                â”‚
â”‚    - å‚æ•°éªŒè¯ (Pydantic)                     â”‚
â”‚    - è°ƒç”¨æœåŠ¡å±‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    æœåŠ¡å±‚ (Services)                         â”‚
â”‚    - ä¸šåŠ¡é€»è¾‘                                â”‚
â”‚    - ä»»åŠ¡ç®¡ç†                                â”‚
â”‚    - èµ„æºè°ƒåº¦                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    æ ¸å¿ƒå±‚ (Core)                             â”‚
â”‚    - é…ç½®ç®¡ç† (Settings)                     â”‚
â”‚    - èµ„æºç®¡ç† (ResourceManager)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    å·¥å…·å±‚ (Utils)                            â”‚
â”‚    - è®­ç»ƒå™¨ (Trainer)                        â”‚
â”‚    - æ¨ç†å™¨ (Benchmark)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å“åº” (JSON)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®¾è®¡åŸåˆ™

1. **å•ä¸€èŒè´£** - æ¯ä¸ªæ¨¡å—åªåšä¸€ä»¶äº‹
2. **ä¾èµ–å€’ç½®** - é«˜å±‚æ¨¡å—ä¸ä¾èµ–ä½å±‚æ¨¡å—
3. **æ¥å£éš”ç¦»** - å®¢æˆ·ç«¯ä¸ä¾èµ–ä¸éœ€è¦çš„æ¥å£
4. **å¼€é—­åŸåˆ™** - å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­

---

## GPUè®¾å¤‡é€‰æ‹©

### è‡ªåŠ¨é€‰æ‹©ç®—æ³•

ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€å°çš„GPUï¼š
```python
def _select_best_gpu(self, task_type: str) -> str:
    """é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU"""
    best_gpu = None
    min_load = float('inf')
    
    for i in range(self.gpu_count):
        gpu_device = f"cuda:{i}"
        current_load = self.device_usage[gpu_device][task_type]
        if current_load < min_load:
            min_load = current_load
            best_gpu = gpu_device
    
    return best_gpu
```

### ä½¿ç”¨åœºæ™¯

#### å•GPUç¯å¢ƒ
```python
# ä½¿ç”¨ cuda æˆ– cuda:0 éƒ½å¯ä»¥
device = "cuda"  # æ¨è
```

#### åŒGPU - è®­ç»ƒæ¨ç†åˆ†ç¦»
```python
# è®­ç»ƒå›ºå®šåœ¨GPU 0ï¼ˆæ˜¾å­˜å¤§ï¼‰
train_device = "cuda:0"

# æ¨ç†å›ºå®šåœ¨GPU 1ï¼ˆæ˜¾å­˜å°ä½†è¶³å¤Ÿï¼‰
infer_device = "cuda:1"
```

#### å¤šGPU - è‡ªåŠ¨è´Ÿè½½å‡è¡¡
```python
# æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ cudaï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ†é…
device = "cuda"  # ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU
```

### GPUä¿¡æ¯æŸ¥è¯¢

```bash
# æŸ¥çœ‹GPUè¯¦ç»†ä¿¡æ¯
curl http://localhost:8000/api/v2/resources/gpu

# æŸ¥çœ‹èµ„æºä½¿ç”¨çŠ¶æ€
curl http://localhost:8000/api/v2/resources
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "available": true,
  "count": 2,
  "devices": [
    {
      "id": 0,
      "device_name": "cuda:0",
      "name": "NVIDIA GeForce RTX 3090",
      "total_memory_gb": 24.0,
      "free_memory_gb": 15.5,
      "utilization": 35.4,
      "current_tasks": {
        "training": 1,
        "inference": 2
      }
    }
  ]
}
```

---

## APIæ¥å£

### å®Œæ•´è·¯ç”±è¡¨

#### è®­ç»ƒæ¥å£
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v2/training/start` | POST | å¯åŠ¨è®­ç»ƒ |
| `/api/v2/training/{id}` | GET | æŸ¥è¯¢çŠ¶æ€ï¼ˆå«è¯¦ç»†æŒ‡æ ‡ï¼‰|
| `/api/v2/training/{id}/logs` | GET | æ—¥å¿—æµï¼ˆå«è®­ç»ƒæŒ‡æ ‡ï¼‰|
| `/api/v2/training/{id}/stop` | POST | åœæ­¢è®­ç»ƒ |

#### æ•°æ®é¢„å¤„ç†æ¥å£ â­æ–°
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v2/preprocessing/split` | POST | æ•°æ®é›†åˆ†å‰² |
| `/api/v2/preprocessing/augment` | POST | æ•°æ®å¢å¼º |
| `/api/v2/preprocessing/crop` | POST | å›¾åƒè£å‰ª |
| `/api/v2/preprocessing/{id}` | GET | æŸ¥è¯¢çŠ¶æ€ |
| `/api/v2/preprocessing/{id}/logs` | GET | æ—¥å¿—æµ |

#### æ¨ç†æ¥å£
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v2/inference/start` | POST | å¯åŠ¨æ¨ç† |
| `/api/v2/inference/batch` | POST | æ‰¹é‡æ¨ç† |
| `/api/v2/inference/{id}` | GET | æŸ¥è¯¢çŠ¶æ€ |

#### ä»»åŠ¡ç®¡ç†
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v2/tasks` | GET | æ‰€æœ‰ä»»åŠ¡ |
| `/api/v2/tasks/{id}` | GET | ä»»åŠ¡è¯¦æƒ… |
| `/api/v2/tasks/{id}/logs` | GET | ä»»åŠ¡æ—¥å¿— |
| `/api/v2/tasks/{id}/cancel` | POST | å–æ¶ˆä»»åŠ¡ |
| `/api/v2/tasks/{id}` | DELETE | åˆ é™¤ä»»åŠ¡ |

#### èµ„æºç®¡ç†
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v2/resources` | GET | èµ„æºçŠ¶æ€ |
| `/api/v2/resources/gpu` | GET | GPUä¿¡æ¯ |
| `/api/v2/resources/config` | POST | æ›´æ–°é…ç½® |

#### ç³»ç»ŸçŠ¶æ€
| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/api/v1/health` | GET | å¥åº·æ£€æŸ¥ |
| `/api/v1/info` | GET | ç³»ç»Ÿä¿¡æ¯ |

### è¯¦ç»†APIå‚æ•°è¯´æ˜

#### 1. å¯åŠ¨è®­ç»ƒ `POST /api/v2/training/start`

**è¯·æ±‚å‚æ•° (TrainingRequest)**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `model` | string | âœ… | - | æ¨¡å‹åç§° (resnet18/resnet50/vit_b_16ç­‰) |
| `num_classes` | integer | âœ… | - | åˆ†ç±»ç±»åˆ«æ•° |
| `train_path` | string | âœ… | - | è®­ç»ƒé›†è·¯å¾„ |
| `val_path` | string | âœ… | - | éªŒè¯é›†è·¯å¾„ |
| `save_path` | string | âœ… | - | æ¨¡å‹ä¿å­˜è·¯å¾„ |
| `batch_size` | integer | âŒ | 8 | æ‰¹æ¬¡å¤§å° (â‰¥1) |
| `num_epochs` | integer | âŒ | 100 | è®­ç»ƒè½®æ•° (â‰¥1) |
| `learning_rate` | float | âŒ | 0.0001 | å­¦ä¹ ç‡ (>0) |
| `image_size` | integer | âŒ | 224 | å›¾åƒå°ºå¯¸ (â‰¥32) |
| `device` | string | âŒ | "cuda" | è®¾å¤‡ (cpu/cuda/cuda:0/cuda:1/...) |
| `weight_path` | string | âŒ | "" | é¢„è®­ç»ƒæƒé‡è·¯å¾„ |
| `pretrained` | boolean | âŒ | true | æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ |
| `shuffle` | boolean | âŒ | true | æ˜¯å¦æ‰“ä¹±æ•°æ® |
| `task_id` | string | âŒ | null | è‡ªå®šä¹‰ä»»åŠ¡ID |
| `priority` | integer | âŒ | 5 | ä¼˜å…ˆçº§ (1-10) |
| `description` | string | âŒ | null | ä»»åŠ¡æè¿° |

**å“åº”å­—æ®µ (TaskResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `task_id` | string | ä»»åŠ¡å”¯ä¸€ID |
| `task_type` | string | ä»»åŠ¡ç±»å‹ (training) |
| `status` | string | ä»»åŠ¡çŠ¶æ€ (pending/running/completed/failed/cancelled) |
| `message` | string | çŠ¶æ€æ¶ˆæ¯ |
| `progress` | integer | è¿›åº¦ç™¾åˆ†æ¯” (0-100) |
| `device` | string | å®é™…ä½¿ç”¨çš„è®¾å¤‡ |
| `priority` | integer | ä¼˜å…ˆçº§ |
| `current_epoch` | integer | å½“å‰è®­ç»ƒè½®æ¬¡ â­æ–° |
| `total_epochs` | integer | æ€»è®­ç»ƒè½®æ¬¡ â­æ–° |
| `latest_metrics` | object | æœ€æ–°è®­ç»ƒæŒ‡æ ‡ï¼ˆè¯¦è§ä¸‹è¡¨ï¼‰â­æ–° |
| `created_at` | string | åˆ›å»ºæ—¶é—´ (ISO 8601æ ¼å¼) |
| `updated_at` | string | æ›´æ–°æ—¶é—´ (ISO 8601æ ¼å¼) |

**latest_metrics å­—æ®µè¯´æ˜ï¼ˆ15+ç§è®­ç»ƒæŒ‡æ ‡ï¼‰** â­æ–°

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `epoch` | integer | å½“å‰epoch |
| `total_epochs` | integer | æ€»epochæ•° |
| `train_loss` | float | è®­ç»ƒæŸå¤± |
| `train_acc` | float | è®­ç»ƒå‡†ç¡®ç‡(%) |
| `val_loss` | float | éªŒè¯æŸå¤± |
| `val_acc` | float | éªŒè¯å‡†ç¡®ç‡(%) |
| `macro_f1` | float | Macro F1åˆ†æ•° |
| `micro_f1` | float | Micro F1åˆ†æ•° |
| `mAP` | float | å¹³å‡ç²¾åº¦ |
| `top1_acc` | float | Top-1å‡†ç¡®ç‡ |
| `top3_acc` | float | Top-3å‡†ç¡®ç‡ |
| `top5_acc` | float | Top-5å‡†ç¡®ç‡ |
| `precision` | float | ç²¾ç¡®åº¦ |
| `recall` | float | å¬å›ç‡ |
| `learning_rate` | float | å½“å‰å­¦ä¹ ç‡ |
| `best_acc` | float | æœ€ä½³å‡†ç¡®ç‡(%) |

**ç¤ºä¾‹**
```bash
curl -X POST "http://localhost:8000/api/v2/training/start" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "resnet18",
    "num_classes": 37,
    "train_path": "data/train",
    "val_path": "data/val",
    "save_path": "models/output",
    "device": "cuda:0",
    "batch_size": 16,
    "num_epochs": 50
  }'
```

å“åº”ï¼š
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "task_type": "training",
  "status": "running",
  "message": "è®­ç»ƒä¸­...",
  "progress": 30,
  "device": "cuda:0",
  "priority": 5,
  "current_epoch": 3,
  "total_epochs": 10,
  "latest_metrics": {
    "epoch": 3,
    "train_loss": 0.5234,
    "train_acc": 82.45,
    "val_loss": 0.6123,
    "val_acc": 78.92,
    "macro_f1": 0.7654,
    "mAP": 0.8123,
    "top1_acc": 78.92,
    "top3_acc": 92.34,
    "top5_acc": 96.78,
    "best_acc": 79.12,
    "learning_rate": 0.0001
  },
  "created_at": "2024-01-01T00:00:00",
  "updated_at": "2024-01-01T00:03:00"
}
```

---

#### 2. å¯åŠ¨æ¨ç† `POST /api/v2/inference/start`

**è¯·æ±‚å‚æ•° (InferenceRequest)**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `cfg_path` | string | âœ… | - | é…ç½®æ–‡ä»¶è·¯å¾„ (.yaml) |
| `weight_path` | string | âœ… | - | æ¨¡å‹æƒé‡è·¯å¾„ (.pth) |
| `source_path` | string | âœ… | - | æ¨ç†æ•°æ®è·¯å¾„ |
| `save_path` | string | âŒ | null | ç»“æœä¿å­˜è·¯å¾„ |
| `device` | string | âŒ | "cuda" | æ¨ç†è®¾å¤‡ (cpu/cuda/cuda:0/cuda:1/...) |
| `task_id` | string | âŒ | null | è‡ªå®šä¹‰ä»»åŠ¡ID |
| `priority` | integer | âŒ | 3 | ä¼˜å…ˆçº§ (1-10) |

**å“åº”å­—æ®µ (TaskResponse)**

åŒè®­ç»ƒæ¥å£ï¼Œ`task_type` ä¸º "inference"

**ç¤ºä¾‹**
```bash
curl -X POST "http://localhost:8000/api/v2/inference/start" \
  -H "Content-Type: application/json" \
  -d '{
    "cfg_path": "configs/model.yaml",
    "weight_path": "models/best.pth",
    "source_path": "data/test",
    "device": "cuda:1"
  }'
```

---

#### 3. æ‰¹é‡æ¨ç† `POST /api/v2/inference/batch`

**è¯·æ±‚å‚æ•° (BatchInferenceRequest)**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `cfg_path` | string | âœ… | - | é…ç½®æ–‡ä»¶è·¯å¾„ |
| `weight_path` | string | âœ… | - | æ¨¡å‹æƒé‡è·¯å¾„ |
| `source_paths` | array[string] | âœ… | - | æ•°æ®è·¯å¾„åˆ—è¡¨ |
| `save_base_path` | string | âŒ | null | ç»“æœä¿å­˜åŸºç¡€è·¯å¾„ |
| `device` | string | âŒ | "cuda" | æ¨ç†è®¾å¤‡ |
| `priority` | integer | âŒ | 3 | ä¼˜å…ˆçº§ |

**å“åº”å­—æ®µ (BatchInferenceResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `status` | string | çŠ¶æ€ (success) |
| `message` | string | æ¶ˆæ¯ |
| `task_ids` | array[string] | æ‰€æœ‰ä»»åŠ¡IDåˆ—è¡¨ |
| `total` | integer | ä»»åŠ¡æ€»æ•° |

**ç¤ºä¾‹**
```bash
curl -X POST "http://localhost:8000/api/v2/inference/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "cfg_path": "configs/model.yaml",
    "weight_path": "models/best.pth",
    "source_paths": ["data/test1", "data/test2", "data/test3"],
    "device": "cuda"
  }'
```

å“åº”ï¼š
```json
{
  "status": "success",
  "message": "å·²å¯åŠ¨ 3 ä¸ªæ¨ç†ä»»åŠ¡",
  "task_ids": ["task-1", "task-2", "task-3"],
  "total": 3
}
```

---

#### 4. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ `GET /api/v2/tasks/{task_id}`

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”å­—æ®µ (TaskResponse)**

åŒå¯åŠ¨è®­ç»ƒæ¥å£çš„å“åº”

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v2/tasks/550e8400-e29b-41d4-a716-446655440000
```

---

#### 5. è·å–æ‰€æœ‰ä»»åŠ¡ `GET /api/v2/tasks`

**æŸ¥è¯¢å‚æ•°**

æ— 

**å“åº”å­—æ®µ (TaskListResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `training_tasks` | array[TaskResponse] | è®­ç»ƒä»»åŠ¡åˆ—è¡¨ |
| `inference_tasks` | array[TaskResponse] | æ¨ç†ä»»åŠ¡åˆ—è¡¨ |
| `total_training` | integer | è®­ç»ƒä»»åŠ¡æ€»æ•° |
| `total_inference` | integer | æ¨ç†ä»»åŠ¡æ€»æ•° |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v2/tasks
```

å“åº”ï¼š
```json
{
  "training_tasks": [...],
  "inference_tasks": [...],
  "total_training": 5,
  "total_inference": 10
}
```

---

#### 6. åœæ­¢è®­ç»ƒ `POST /api/v2/training/{task_id}/stop`

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”å­—æ®µ (TaskActionResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `status` | string | çŠ¶æ€ (success/error) |
| `message` | string | æ“ä½œç»“æœæ¶ˆæ¯ |
| `task_id` | string | ä»»åŠ¡ID |

**ç¤ºä¾‹**
```bash
curl -X POST http://localhost:8000/api/v2/training/task-123/stop
```

å“åº”ï¼š
```json
{
  "status": "success",
  "message": "è®­ç»ƒä»»åŠ¡å·²åœæ­¢",
  "task_id": "task-123"
}
```

---

#### 7. å–æ¶ˆä»»åŠ¡ `POST /api/v2/tasks/{task_id}/cancel`

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”å­—æ®µ (TaskActionResponse)**

åŒåœæ­¢è®­ç»ƒæ¥å£

**ç¤ºä¾‹**
```bash
curl -X POST http://localhost:8000/api/v2/tasks/task-123/cancel
```

---

#### 8. åˆ é™¤ä»»åŠ¡ `DELETE /api/v2/tasks/{task_id}`

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”å­—æ®µ (TaskActionResponse)**

åŒåœæ­¢è®­ç»ƒæ¥å£

**ç¤ºä¾‹**
```bash
curl -X DELETE http://localhost:8000/api/v2/tasks/task-123
```

---

#### 9. è·å–ä»»åŠ¡æ—¥å¿— `GET /api/v2/tasks/{task_id}/logs`

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”**

è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `timestamp` | string | æ—¶é—´æˆ³ |
| `level` | string | æ—¥å¿—çº§åˆ« (INFO/WARNING/ERROR) |
| `message` | string | æ—¥å¿—æ¶ˆæ¯ |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v2/tasks/task-123/logs
```

å“åº”ï¼š
```json
[
  {
    "timestamp": "2024-01-01T00:00:00",
    "level": "INFO",
    "message": "è®­ç»ƒå¼€å§‹"
  },
  {
    "timestamp": "2024-01-01T00:01:00",
    "level": "INFO",
    "message": "Epoch 1/50, Loss: 2.345"
  }
]
```

---

#### 10. è·å–è®­ç»ƒå®æ—¶æ—¥å¿—æµï¼ˆå«è¯¦ç»†æŒ‡æ ‡ï¼‰`GET /api/v2/training/{task_id}/logs` â­æ–°

**è·¯å¾„å‚æ•°**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `task_id` | string | âœ… | ä»»åŠ¡ID |

**å“åº”**

Server-Sent Events (SSE) æµï¼Œæ¯æ¡æ¶ˆæ¯åŒ…å«ï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `timestamp` | string | æ—¶é—´æˆ³ |
| `level` | string | æ—¥å¿—çº§åˆ« |
| `message` | string | æ—¥å¿—æ¶ˆæ¯ |
| `metrics` | object | è®­ç»ƒæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰â­ |
| `step` | integer | è®­ç»ƒæ­¥æ•°ï¼ˆå¦‚æœæœ‰ï¼‰â­ |
| `stage` | string | è®­ç»ƒé˜¶æ®µï¼ˆepoch_start/training/validation/epoch_end/completedï¼‰â­ |

**æ—¥å¿—ç¤ºä¾‹ï¼ˆåŸºç¡€æ—¥å¿—ï¼‰**ï¼š
```json
data: {"timestamp": "2024-01-01T00:00:00", "level": "INFO", "message": "è®­ç»ƒå¼€å§‹..."}
```

**æ—¥å¿—ç¤ºä¾‹ï¼ˆå«è®­ç»ƒæŒ‡æ ‡ï¼‰** â­æ–°ï¼š
```json
data: {
  "timestamp": "2024-01-01T00:01:00",
  "level": "INFO",
  "message": "Train Loss: 0.5234, Train Accuracy: 82.45%",
  "metrics": {
    "epoch": 3,
    "train_loss": 0.5234,
    "train_acc": 82.45
  },
  "stage": "training"
}
```

**æ—¥å¿—ç¤ºä¾‹ï¼ˆå«éªŒè¯æŒ‡æ ‡ï¼‰** â­æ–°ï¼š
```json
data: {
  "timestamp": "2024-01-01T00:02:00",
  "level": "INFO",
  "message": "Validation Loss: 0.6123, Validation Accuracy: 78.92%",
  "metrics": {
    "epoch": 3,
    "val_loss": 0.6123,
    "val_acc": 78.92,
    "macro_f1": 0.7654,
    "mAP": 0.8123,
    "top1_acc": 78.92
  },
  "stage": "validation"
}
```

**ç¤ºä¾‹ (Python) - åŸºç¡€ç”¨æ³•**
```python
import requests
import json

url = f"http://localhost:8000/api/v2/training/task-123/logs"
response = requests.get(url, stream=True)

for line in response.iter_lines():
    if line:
        line_str = line.decode('utf-8')
        if line_str.startswith('data: '):
            log_entry = json.loads(line_str[6:])
            print(f"[{log_entry['level']}] {log_entry['message']}")
```

**ç¤ºä¾‹ (Python) - å«æŒ‡æ ‡å¤„ç†** â­æ–°
```python
import requests
import json

url = f"http://localhost:8000/api/v2/training/task-123/logs"
response = requests.get(url, stream=True)

for line in response.iter_lines():
    if line:
        line_str = line.decode('utf-8')
        if line_str.startswith('data: '):
            log_entry = json.loads(line_str[6:])
            
            # æ˜¾ç¤ºåŸºæœ¬æ—¥å¿—
            print(f"[{log_entry['level']}] {log_entry['message']}")
            
            # å¦‚æœæœ‰è®­ç»ƒæŒ‡æ ‡ï¼Œè¯¦ç»†æ˜¾ç¤º
            if log_entry.get('metrics'):
                metrics = log_entry['metrics']
                stage = log_entry.get('stage', '')
                print(f"  â””â”€ é˜¶æ®µ: {stage}")
                
                if metrics.get('train_acc'):
                    print(f"  â””â”€ è®­ç»ƒå‡†ç¡®ç‡: {metrics['train_acc']}%")
                if metrics.get('val_acc'):
                    print(f"  â””â”€ éªŒè¯å‡†ç¡®ç‡: {metrics['val_acc']}%")
                if metrics.get('macro_f1'):
                    print(f"  â””â”€ Macro F1: {metrics['macro_f1']}")
```

---

#### 11. è·å–èµ„æºçŠ¶æ€ `GET /api/v2/resources`

**æŸ¥è¯¢å‚æ•°**

æ— 

**å“åº”å­—æ®µ (ResourceStatusResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `device_usage` | object | å„è®¾å¤‡å½“å‰ä½¿ç”¨æƒ…å†µ {è®¾å¤‡å: {training: æ•°é‡, inference: æ•°é‡}} |
| `active_tasks` | object | å„è®¾å¤‡æ´»è·ƒä»»åŠ¡åˆ—è¡¨ {è®¾å¤‡å: [ä»»åŠ¡åˆ—è¡¨]} |
| `limits` | object | å„è®¾å¤‡å¹¶å‘é™åˆ¶ {è®¾å¤‡å: {training: æœ€å¤§å€¼, inference: æœ€å¤§å€¼}} |
| `gpu_info` | object | GPUè¯¦ç»†ä¿¡æ¯ |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v2/resources
```

å“åº”ï¼š
```json
{
  "device_usage": {
    "cuda:0": {"training": 1, "inference": 2},
    "cuda:1": {"training": 0, "inference": 3}
  },
  "active_tasks": {
    "cuda:0": [
      {"id": "task-1", "type": "training"},
      {"id": "task-2", "type": "inference"}
    ]
  },
  "limits": {
    "cuda:0": {"training": 1, "inference": 3}
  },
  "gpu_info": {...}
}
```

---

#### 12. è·å–GPUä¿¡æ¯ `GET /api/v2/resources/gpu`

**æŸ¥è¯¢å‚æ•°**

æ— 

**å“åº”å­—æ®µ**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `available` | boolean | GPUæ˜¯å¦å¯ç”¨ |
| `count` | integer | GPUæ•°é‡ |
| `cuda_version` | string | CUDAç‰ˆæœ¬ |
| `pytorch_version` | string | PyTorchç‰ˆæœ¬ |
| `devices` | array[object] | GPUè®¾å¤‡åˆ—è¡¨ |

**devices æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ **

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | integer | GPU ID |
| `device_name` | string | è®¾å¤‡åç§° (cuda:0/cuda:1) |
| `name` | string | GPUå‹å·åç§° |
| `compute_capability` | string | è®¡ç®—èƒ½åŠ› |
| `total_memory_gb` | float | æ€»æ˜¾å­˜ (GB) |
| `allocated_memory_gb` | float | å·²åˆ†é…æ˜¾å­˜ (GB) |
| `cached_memory_gb` | float | ç¼“å­˜æ˜¾å­˜ (GB) |
| `free_memory_gb` | float | ç©ºé—²æ˜¾å­˜ (GB) |
| `utilization` | float | åˆ©ç”¨ç‡ (%) |
| `current_tasks` | object | å½“å‰ä»»åŠ¡æ•° {training: æ•°é‡, inference: æ•°é‡} |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v2/resources/gpu
```

å“åº”ï¼š
```json
{
  "available": true,
  "count": 2,
  "cuda_version": "11.8",
  "pytorch_version": "2.0.1",
  "devices": [
    {
      "id": 0,
      "device_name": "cuda:0",
      "name": "NVIDIA GeForce RTX 3090",
      "compute_capability": "8.6",
      "total_memory_gb": 24.0,
      "allocated_memory_gb": 8.5,
      "cached_memory_gb": 0.3,
      "free_memory_gb": 15.5,
      "utilization": 35.4,
      "current_tasks": {
        "training": 1,
        "inference": 2
      }
    }
  ]
}
```

---

#### 13. æ›´æ–°èµ„æºé…ç½® `POST /api/v2/resources/config`

**è¯·æ±‚å‚æ•° (ResourceConfigUpdate)**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `max_concurrent` | object | âŒ | å¹¶å‘é™åˆ¶é…ç½® {è®¾å¤‡å: {training: æ•°é‡, inference: æ•°é‡}} |

**å“åº”å­—æ®µ (ConfigUpdateResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `status` | string | çŠ¶æ€ (success) |
| `message` | string | æ¶ˆæ¯ |
| `current_config` | object | æ›´æ–°åçš„é…ç½® |

**ç¤ºä¾‹**
```bash
curl -X POST "http://localhost:8000/api/v2/resources/config" \
  -H "Content-Type: application/json" \
  -d '{
    "max_concurrent": {
      "cuda:0": {"training": 2, "inference": 5}
    }
  }'
```

å“åº”ï¼š
```json
{
  "status": "success",
  "message": "èµ„æºé…ç½®å·²æ›´æ–°",
  "current_config": {
    "cuda:0": {"training": 2, "inference": 5},
    "cuda:1": {"training": 1, "inference": 3}
  }
}
```

---

#### 14. å¥åº·æ£€æŸ¥ `GET /api/v1/health`

**æŸ¥è¯¢å‚æ•°**

æ— 

**å“åº”å­—æ®µ (HealthResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `status` | string | æœåŠ¡çŠ¶æ€ (healthy/unhealthy) |
| `timestamp` | string | æ£€æŸ¥æ—¶é—´ |
| `version` | string | æœåŠ¡ç‰ˆæœ¬ |
| `training_tasks` | integer | è®­ç»ƒä»»åŠ¡æ•° |
| `inference_tasks` | integer | æ¨ç†ä»»åŠ¡æ•° |
| `active_log_streams` | integer | æ´»è·ƒæ—¥å¿—æµæ•° |
| `resource_status` | object | èµ„æºçŠ¶æ€æ‘˜è¦ |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v1/health
```

å“åº”ï¼š
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T00:00:00",
  "version": "2.3.1",
  "training_tasks": 2,
  "inference_tasks": 5,
  "active_log_streams": 3,
  "resource_status": {
    "gpu_available": true,
    "gpu_count": 2
  }
}
```

---

#### 15. ç³»ç»Ÿä¿¡æ¯ `GET /api/v1/info`

**æŸ¥è¯¢å‚æ•°**

æ— 

**å“åº”å­—æ®µ (InfoResponse)**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `app_name` | string | åº”ç”¨åç§° |
| `version` | string | ç‰ˆæœ¬å· |
| `environment` | string | è¿è¡Œç¯å¢ƒ (development/production) |
| `supported_models` | array[string] | æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ |
| `resource_limits` | object | èµ„æºé™åˆ¶é…ç½® |
| `gpu_available` | boolean | GPUæ˜¯å¦å¯ç”¨ |

**ç¤ºä¾‹**
```bash
curl http://localhost:8000/api/v1/info
```

å“åº”ï¼š
```json
{
  "app_name": "RFUAV Model Service",
  "version": "2.3.1",
  "environment": "production",
  "supported_models": [
    "resnet18", "resnet50", "vit_b_16", "swin_v2_t"
  ],
  "resource_limits": {
    "max_training_concurrent_gpu": 1,
    "max_inference_concurrent_gpu": 3
  },
  "gpu_available": true
}
```

---

## JSONæ ¼å¼è§„èŒƒ

### è¯·æ±‚æ ¼å¼

æ‰€æœ‰è¯·æ±‚å¿…é¡»ï¼š
- Content-Type: `application/json`
- è¯·æ±‚ä½“ä¸ºæœ‰æ•ˆçš„JSON
- å­—ç¬¦ä¸²ä½¿ç”¨åŒå¼•å·

### å“åº”æ ¼å¼

æ‰€æœ‰å“åº”ä¿è¯ï¼š
- Content-Type: `application/json`
- å“åº”ä½“ç¬¦åˆPydanticæ¨¡å‹å®šä¹‰
- ç±»å‹å®‰å…¨ï¼Œè‡ªåŠ¨éªŒè¯

### å“åº”æ¨¡å‹

#### TaskResponse - ä»»åŠ¡å“åº”
```json
{
  "task_id": "xxx",
  "task_type": "training",
  "status": "running",
  "message": "è®­ç»ƒä¸­...",
  "progress": 45,
  "device": "cuda:0",
  "priority": 5,
  "created_at": "2024-01-01T00:00:00",
  "updated_at": "2024-01-01T00:05:00"
}
```

#### TaskActionResponse - æ“ä½œå“åº”
```json
{
  "status": "success",
  "message": "ä»»åŠ¡å·²åœæ­¢",
  "task_id": "xxx"
}
```

#### ResourceStatusResponse - èµ„æºçŠ¶æ€
```json
{
  "device_usage": {"cuda:0": {"training": 1, "inference": 2}},
  "active_tasks": {...},
  "limits": {...},
  "gpu_info": {...}
}
```

---

## Webæµ‹è¯•ç•Œé¢

### åŠŸèƒ½ç‰¹ç‚¹

æ‰“å¼€ `test_web_ui.html` å¯ä»¥ï¼š
- âœ… æŸ¥çœ‹GPUç¡¬ä»¶ä¿¡æ¯
- âœ… ç›‘æ§èµ„æºä½¿ç”¨çŠ¶æ€
- âœ… å¯åŠ¨è®­ç»ƒä»»åŠ¡
- âœ… å¯åŠ¨æ¨ç†ä»»åŠ¡
- âœ… æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨
- âœ… å®æ—¶æ—¥å¿—æµ

### ä½¿ç”¨æ–¹æ³•

1. **å¯åŠ¨æœåŠ¡**
```bash
python app_refactored.py
```

2. **æ‰“å¼€ç•Œé¢**
åŒå‡» `test_web_ui.html` æˆ–åœ¨æµè§ˆå™¨æ‰“å¼€

3. **æµ‹è¯•åŠŸèƒ½**
- ç‚¹å‡»"åˆ·æ–°GPUä¿¡æ¯"æŸ¥çœ‹ç¡¬ä»¶
- é€‰æ‹©è®¾å¤‡ï¼ˆcuda/cuda:0/cuda:1/cpuï¼‰
- å¡«å†™è®­ç»ƒå‚æ•°
- ç‚¹å‡»"å¯åŠ¨è®­ç»ƒ"
- æŸ¥çœ‹å®æ—¶æ—¥å¿—

### ç•Œé¢æˆªå›¾è¯´æ˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RFUAV Model Service - æµ‹è¯•é¢æ¿          â”‚
â”‚  âœ… æœåŠ¡è¿æ¥æ­£å¸¸                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUç¡¬ä»¶ä¿¡æ¯     â”‚  èµ„æºä½¿ç”¨çŠ¶æ€         â”‚
â”‚  [åˆ·æ–°æŒ‰é’®]      â”‚  [åˆ·æ–°æŒ‰é’®]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¯åŠ¨è®­ç»ƒä»»åŠ¡    â”‚  å¯åŠ¨æ¨ç†ä»»åŠ¡         â”‚
â”‚  - è®¾å¤‡é€‰æ‹© â­   â”‚  - è®¾å¤‡é€‰æ‹© â­        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä»»åŠ¡åˆ—è¡¨        â”‚  å®æ—¶æ—¥å¿—             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# åº”ç”¨é…ç½®
APP_NAME="RFUAV Model Service"
VERSION="2.3.1"
ENVIRONMENT="production"

# æœåŠ¡å™¨é…ç½®
HOST="0.0.0.0"
PORT=8000
DEBUG=false

# èµ„æºé™åˆ¶
MAX_TRAINING_CONCURRENT_GPU=1
MAX_INFERENCE_CONCURRENT_GPU=3
MAX_TRAINING_CONCURRENT_CPU=2
MAX_INFERENCE_CONCURRENT_CPU=4

# ä»»åŠ¡é…ç½®
DEFAULT_TRAIN_PRIORITY=5
DEFAULT_INFERENCE_PRIORITY=3
```

### è¿è¡Œæ—¶æ›´æ–°

```bash
curl -X POST "http://localhost:8000/api/v2/resources/config" \
  -H "Content-Type: application/json" \
  -d '{
    "max_concurrent": {
      "cuda:0": {"training": 2, "inference": 5}
    }
  }'
```

---

## ä½¿ç”¨ç¤ºä¾‹

### Pythonå®Œæ•´ç¤ºä¾‹

```python
from test_refactored_api import RFUAVClient
import time

# åˆ›å»ºå®¢æˆ·ç«¯
client = RFUAVClient("http://localhost:8000")

# 1. æŸ¥çœ‹GPUä¿¡æ¯
gpu_info = client.get_gpu_info()
print(f"GPU Count: {gpu_info['count']}")

# 2. å¯åŠ¨è®­ç»ƒï¼ˆè‡ªåŠ¨é€‰æ‹©GPUï¼‰
train_result = client.start_training(
    model="resnet18",
    num_classes=37,
    train_path="data/train",
    val_path="data/val",
    save_path="models/output",
    device="cuda",  # è‡ªåŠ¨é€‰æ‹©
    batch_size=16,
    num_epochs=50
)
train_task_id = train_result['task_id']
print(f"Training started: {train_task_id}")
print(f"Using device: {train_result['device']}")

# 3. å¯åŠ¨æ¨ç†ï¼ˆæŒ‡å®šGPU 1ï¼‰
infer_result = client.start_inference(
    cfg_path="configs/model.yaml",
    weight_path="models/best.pth",
    source_path="data/test",
    device="cuda:1"  # æŒ‡å®šGPU 1
)
infer_task_id = infer_result['task_id']
print(f"Inference started: {infer_task_id}")

# 4. ç›‘æ§ä»»åŠ¡
while True:
    status = client.get_task(train_task_id)
    print(f"Status: {status['status']}, Progress: {status['progress']}%")
    
    if status['status'] in ['completed', 'failed', 'cancelled']:
        break
    
    time.sleep(5)

# 5. æŸ¥çœ‹èµ„æº
resources = client.get_resources()
print(f"Resource usage: {resources['device_usage']}")

# 6. è·å–æ‰€æœ‰ä»»åŠ¡
tasks = client.get_all_tasks()
print(f"Total tasks: Training={tasks['total_training']}, Inference={tasks['total_inference']}")
```

### cURLç¤ºä¾‹

```bash
# 1. å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health

# 2. æŸ¥çœ‹GPU
curl http://localhost:8000/api/v2/resources/gpu

# 3. å¯åŠ¨è®­ç»ƒ
curl -X POST http://localhost:8000/api/v2/training/start \
  -H "Content-Type: application/json" \
  -d '{"model":"resnet18","device":"cuda:0",...}'

# 4. æŸ¥è¯¢çŠ¶æ€
curl http://localhost:8000/api/v2/tasks/{task_id}

# 5. åœæ­¢ä»»åŠ¡
curl -X POST http://localhost:8000/api/v2/training/{task_id}/stop
```

---

## å¹¶å‘å®‰å…¨ä¿è¯

### æ¶æ„è®¾è®¡

ç³»ç»Ÿé‡‡ç”¨**å®Œå…¨éé˜»å¡çš„å¼‚æ­¥æ¶æ„**ï¼Œç¡®ä¿é«˜å¹¶å‘åœºæ™¯ä¸‹ä¸ä¼šå‡ºç°é˜»å¡ï¼š

```
å®¢æˆ·ç«¯è¯·æ±‚ â†’ FastAPI (async) â†’ åå°ä»»åŠ¡ (éé˜»å¡) â†’ å·¥ä½œçº¿ç¨‹æ± 
                                            â†“
                                      èµ„æºç®¡ç†å™¨ (çº¿ç¨‹å®‰å…¨)
```

### å…³é”®ç‰¹æ€§

#### 1. APIå±‚å®Œå…¨å¼‚æ­¥ âœ…
- æ‰€æœ‰æ¥å£ä½¿ç”¨ `async/await`
- è¯·æ±‚ç«‹å³è¿”å›ï¼ˆ< 50msï¼‰
- ä¸ä¼šç›¸äº’é˜»å¡

#### 2. åå°ä»»åŠ¡æ‰§è¡Œ âœ…
```python
@router.post("/start")
async def start_training(request, background_tasks):
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_id = await service.start_training(request, background_tasks)
    return {"task_id": task_id}  # ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…è®­ç»ƒå®Œæˆ
```

#### 3. èµ„æºç®¡ç†çº¿ç¨‹å®‰å…¨ âœ…
```python
class ResourceManager:
    def __init__(self):
        self.lock = threading.Lock()
    
    def allocate(self, device, task_type, task_id):
        with self.lock:  # ä¿æŠ¤å…±äº«èµ„æº
            # çº¿ç¨‹å®‰å…¨çš„èµ„æºåˆ†é…
```

#### 4. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ âœ…
- ä»»åŠ¡è‡ªåŠ¨æ’é˜Ÿ
- ä¼˜å…ˆçº§ç®¡ç†
- GPUè´Ÿè½½å‡è¡¡

### å¹¶å‘èƒ½åŠ›

| æ“ä½œç±»å‹ | å¹¶å‘é‡ | å“åº”æ—¶é—´ | è¯´æ˜ |
|---------|--------|---------|------|
| åˆ›å»ºä»»åŠ¡ | 500+/ç§’ | < 50ms | ç«‹å³è¿”å›ä»»åŠ¡ID |
| æŸ¥è¯¢çŠ¶æ€ | 1000+/ç§’ | < 20ms | å†…å­˜è¯»å– |
| è·å–æ—¥å¿— | 100/ç§’ | < 100ms | æ—¥å¿—é˜Ÿåˆ— |
| SSEè¿æ¥ | 100å¹¶å‘ | - | å®æ—¶æ—¥å¿—æµ |

### å¹¶å‘æµ‹è¯•

è¿è¡Œå¹¶å‘æ€§èƒ½æµ‹è¯•ï¼š
```bash
# å®‰è£…ä¾èµ–
pip install aiohttp

# è¿è¡Œæµ‹è¯•
python test_concurrency.py
```

**æµ‹è¯•å†…å®¹**ï¼š
- âœ… å¹¶å‘åˆ›å»º20ä¸ªä»»åŠ¡
- âœ… å¹¶å‘æŸ¥è¯¢100æ¬¡
- âœ… æ··åˆå·¥ä½œè´Ÿè½½
- âœ… èµ„æºçŠ¶æ€ç›‘æ§

**é¢„æœŸç»“æœ**ï¼š
```
æµ‹è¯•1: å¹¶å‘åˆ›å»º 20 ä¸ªè®­ç»ƒä»»åŠ¡
âœ… æˆåŠŸ: 20/20
â±ï¸  æ€»è€—æ—¶: 0.85ç§’
â±ï¸  å¹³å‡å“åº”: 42.50ms
ğŸ“Š QPS: 23.53 è¯·æ±‚/ç§’

æµ‹è¯•2: å¹¶å‘æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ 100 æ¬¡
âœ… æˆåŠŸ: 100/100
â±ï¸  æ€»è€—æ—¶: 0.32ç§’
â±ï¸  å¹³å‡å“åº”: 3.20ms
ğŸ“Š QPS: 312.50 è¯·æ±‚/ç§’
```

### ç”Ÿäº§éƒ¨ç½²é…ç½®

#### æ¨èé…ç½®
```bash
# Uvicornå¤šè¿›ç¨‹éƒ¨ç½²
uvicorn app_refactored:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \              # 4ä¸ªå·¥ä½œè¿›ç¨‹
    --limit-concurrency 1000 \ # æœ€å¤§å¹¶å‘è¿æ¥
    --backlog 2048 \           # è¯·æ±‚é˜Ÿåˆ—å¤§å°
    --timeout-keep-alive 30    # ä¿æŒè¿æ¥30ç§’
```

#### GPUå¹¶å‘é™åˆ¶
```bash
# .env é…ç½®
MAX_TRAINING_CONCURRENT_GPU=1   # æ¯ä¸ªGPUæœ€å¤š1ä¸ªè®­ç»ƒä»»åŠ¡
MAX_INFERENCE_CONCURRENT_GPU=3  # æ¯ä¸ªGPUæœ€å¤š3ä¸ªæ¨ç†ä»»åŠ¡
```

### æ€§èƒ½ç›‘æ§

æŸ¥çœ‹å®æ—¶èµ„æºçŠ¶æ€ï¼š
```bash
# æŸ¥çœ‹GPUä½¿ç”¨
curl http://localhost:8000/api/v2/resources/gpu

# æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—
curl http://localhost:8000/api/v2/resources

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health
```

### è´Ÿè½½èƒ½åŠ›è¯„ä¼°

**å•æœºé…ç½®**ï¼ˆ2Ã—RTX 3090ï¼‰ï¼š
- APIè¯·æ±‚: 1000+ QPS
- åŒæ—¶è¿è¡Œä»»åŠ¡: 8ä¸ªï¼ˆ2 GPU Ã— 4ä»»åŠ¡ï¼‰
- æ’é˜Ÿä»»åŠ¡: æ•°ç™¾ä¸ª
- å“åº”å»¶è¿Ÿ: < 50ms

**æ‰©å±•æ–¹æ¡ˆ**ï¼ˆå¤šæœºéƒ¨ç½²ï¼‰ï¼š
- ä½¿ç”¨Nginxè´Ÿè½½å‡è¡¡
- Rediså…±äº«ä»»åŠ¡é˜Ÿåˆ—
- æ°´å¹³æ‰©å±•è‡³å¤šå°æœåŠ¡å™¨
- 3å°æœåŠ¡å™¨å¯è¾¾ 3000+ QPS

### å®‰å…¨ä¿è¯

| ä¿æŠ¤æœºåˆ¶ | å®ç°æ–¹å¼ | ä½œç”¨ |
|---------|---------|------|
| **å¹¶å‘é™åˆ¶** | ResourceManager | é˜²æ­¢èµ„æºè¿‡è½½ |
| **ä»»åŠ¡æ’é˜Ÿ** | è½®è¯¢ç­‰å¾… | é¿å…GPUæ˜¾å­˜æº¢å‡º |
| **çº¿ç¨‹å®‰å…¨** | threading.Lock | é˜²æ­¢æ•°æ®ç«äº‰ |
| **å¼‚å¸¸éš”ç¦»** | try-catch | å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡ |
| **èµ„æºé‡Šæ”¾** | finallyå— | ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾ |

### è¯¦ç»†åˆ†æ

å®Œæ•´çš„å¹¶å‘å®‰å…¨æ€§åˆ†æå’Œæ€§èƒ½æµ‹è¯•ï¼Œè¯·å‚è€ƒï¼š
- **[å¹¶å‘å®‰å…¨æ€§åˆ†ææŠ¥å‘Š](CONCURRENCY_ANALYSIS.md)** - è¯¦ç»†çš„æ¶æ„åˆ†æ
- **[å¹¶å‘æµ‹è¯•è„šæœ¬](test_concurrency.py)** - å®é™…æ€§èƒ½æµ‹è¯•

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### Q1: æœåŠ¡æ— æ³•å¯åŠ¨
**A**: 
1. æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
2. æ£€æŸ¥Pythonç‰ˆæœ¬ (éœ€è¦3.8+)
3. æ£€æŸ¥ä¾èµ–æ˜¯å¦å®Œæ•´å®‰è£…

```bash
# æ£€æŸ¥ç«¯å£
netstat -ano | findstr :8000

# æ£€æŸ¥ä¾èµ–
pip list | grep fastapi
```

#### Q2: GPUä¸å¯ç”¨
**A**:
1. æ£€æŸ¥CUDAæ˜¯å¦å®‰è£…
2. æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒCUDA
3. ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

#### Q3: ä»»åŠ¡ä¸€ç›´æ’é˜Ÿ
**A**:
1. æŸ¥çœ‹èµ„æºçŠ¶æ€
2. æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡å ç”¨GPU
3. è°ƒæ•´å¹¶å‘é™åˆ¶

```bash
curl http://localhost:8000/api/v2/resources
```

#### Q4: æ˜¾å­˜ä¸è¶³
**A**:
1. é™ä½batch_size
2. ä½¿ç”¨æ˜¾å­˜æ›´å¤§çš„GPU
3. å‡å°‘å¹¶å‘ä»»åŠ¡æ•°

```python
device = "cuda:0"  # ä½¿ç”¨24GBçš„GPU
batch_size = 8     # ä»16é™åˆ°8
```

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -f logs/app.log

# æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
curl http://localhost:8000/api/v2/tasks/{task_id}/logs
```

