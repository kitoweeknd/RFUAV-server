import threading
import torch
from collections import defaultdict
from typing import Dict, List, Optional
import logging

from core.config import settings

logger = logging.getLogger(__name__)


class ResourceManager:
    """èµ„æºç®¡ç†å™¨ - ç®¡ç†GPU/CPUèµ„æºåˆ†é…"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–èµ„æºç®¡ç†å™¨"""
        if hasattr(self, '_initialized'):
            return
            
        # æ£€æµ‹GPUä¿¡æ¯
        self.gpu_available = torch.cuda.is_available()
        self.gpu_count = torch.cuda.device_count() if self.gpu_available else 0
        
        # ä¸ºæ¯ä¸ªGPUè®¾å¤‡åˆ›å»ºèµ„æºè¿½è¸ª
        self.device_usage = defaultdict(lambda: {"training": 0, "inference": 0})
        self.active_tasks = defaultdict(list)
        self.lock = threading.Lock()
        
        # åŸºç¡€é…ç½®
        self.max_concurrent = {
            "cuda": {
                "training": settings.MAX_TRAINING_CONCURRENT_GPU,
                "inference": settings.MAX_INFERENCE_CONCURRENT_GPU
            },
            "cpu": {
                "training": settings.MAX_TRAINING_CONCURRENT_CPU,
                "inference": settings.MAX_INFERENCE_CONCURRENT_CPU
            }
        }
        
        # ä¸ºæ¯ä¸ªGPUè®¾å¤‡è®¾ç½®ç‹¬ç«‹çš„é™åˆ¶
        if self.gpu_count > 1:
            for i in range(self.gpu_count):
                device_name = f"cuda:{i}"
                self.max_concurrent[device_name] = {
                    "training": settings.MAX_TRAINING_CONCURRENT_GPU,
                    "inference": settings.MAX_INFERENCE_CONCURRENT_GPU
                }
        
        self._initialized = True
        logger.info(f"èµ„æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - GPUå¯ç”¨: {self.gpu_available}, GPUæ•°é‡: {self.gpu_count}")
    
    def can_allocate(self, device: str, task_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…èµ„æº"""
        with self.lock:
            # å¤„ç†é€šç”¨cudaè®¾å¤‡ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰
            if device == "cuda":
                # å¦‚æœåªæœ‰ä¸€ä¸ªGPUï¼Œä½¿ç”¨cuda:0
                if self.gpu_count == 1:
                    device = "cuda:0"
                # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªGPUå¯ç”¨
                elif self.gpu_count > 1:
                    for i in range(self.gpu_count):
                        gpu_device = f"cuda:{i}"
                        current = self.device_usage[gpu_device][task_type]
                        max_allowed = self.max_concurrent.get(gpu_device, self.max_concurrent["cuda"])[task_type]
                        if current < max_allowed:
                            return True
                    return False
            
            # æ£€æŸ¥å…·ä½“è®¾å¤‡
            current = self.device_usage[device][task_type]
            if device.startswith("cuda:"):
                # å…·ä½“GPUè®¾å¤‡
                max_allowed = self.max_concurrent.get(device, self.max_concurrent["cuda"])[task_type]
            else:
                # CPUæˆ–é€šç”¨cuda
                max_allowed = self.max_concurrent[device][task_type]
            return current < max_allowed
    
    def _select_best_gpu(self, task_type: str) -> Optional[str]:
        """è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„GPUè®¾å¤‡"""
        if not self.gpu_available or self.gpu_count == 0:
            return None
        
        if self.gpu_count == 1:
            return "cuda:0"
        
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU
        best_gpu = None
        min_load = float('inf')
        
        for i in range(self.gpu_count):
            gpu_device = f"cuda:{i}"
            current_load = self.device_usage[gpu_device][task_type]
            if current_load < min_load:
                min_load = current_load
                best_gpu = gpu_device
        
        return best_gpu
    
    def allocate(self, device: str, task_type: str, task_id: str) -> str:
        """
        åˆ†é…èµ„æº
        è¿”å›å®é™…åˆ†é…çš„è®¾å¤‡åç§°ï¼ˆå¦‚æœæ˜¯cudaä¼šè‡ªåŠ¨é€‰æ‹©å…·ä½“GPUï¼‰
        """
        with self.lock:
            # å¦‚æœæ˜¯é€šç”¨cudaï¼Œè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„GPU
            actual_device = device
            if device == "cuda" and self.gpu_count > 0:
                actual_device = self._select_best_gpu(task_type)
                if actual_device is None:
                    actual_device = "cuda:0"  # å›é€€åˆ°ç¬¬ä¸€ä¸ªGPU
            
            self.device_usage[actual_device][task_type] += 1
            self.active_tasks[actual_device].append({
                "id": task_id,
                "type": task_type
            })
            
            # è·å–é™åˆ¶ä¿¡æ¯
            if actual_device.startswith("cuda:"):
                max_allowed = self.max_concurrent.get(actual_device, self.max_concurrent["cuda"])[task_type]
            else:
                max_allowed = self.max_concurrent[actual_device][task_type]
            
            logger.info(
                f"èµ„æºåˆ†é…: {actual_device} {task_type} (ä»»åŠ¡: {task_id[:8]}) - "
                f"å½“å‰: {self.device_usage[actual_device][task_type]}/{max_allowed}"
            )
            
            return actual_device
    
    def release(self, device: str, task_type: str, task_id: str):
        """é‡Šæ”¾èµ„æº"""
        with self.lock:
            self.device_usage[device][task_type] = max(0, self.device_usage[device][task_type] - 1)
            self.active_tasks[device] = [
                t for t in self.active_tasks[device] if t["id"] != task_id
            ]
            
            # è·å–é™åˆ¶ä¿¡æ¯
            if device.startswith("cuda:"):
                max_allowed = self.max_concurrent.get(device, self.max_concurrent["cuda"])[task_type]
            else:
                max_allowed = self.max_concurrent[device][task_type]
            
            logger.info(
                f"èµ„æºé‡Šæ”¾: {device} {task_type} (ä»»åŠ¡: {task_id[:8]}) - "
                f"å½“å‰: {self.device_usage[device][task_type]}/{max_allowed}"
            )
    
    def get_status(self) -> Dict:
        """è·å–èµ„æºä½¿ç”¨çŠ¶æ€"""
        with self.lock:
            return {
                "device_usage": dict(self.device_usage),
                "active_tasks": dict(self.active_tasks),
                "limits": self.max_concurrent
            }
    
    def get_gpu_info(self) -> Dict:
        """è·å–GPUä¿¡æ¯"""
        gpu_info = {
            "available": torch.cuda.is_available(),
            "count": 0,
            "devices": [],
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "pytorch_version": torch.__version__
        }
        
        if torch.cuda.is_available():
            gpu_info["count"] = torch.cuda.device_count()
            
            for i in range(gpu_info["count"]):
                props = torch.cuda.get_device_properties(i)
                total_memory = props.total_memory / 1024**3
                allocated_memory = torch.cuda.memory_allocated(i) / 1024**3
                cached_memory = torch.cuda.memory_reserved(i) / 1024**3
                
                device_name = f"cuda:{i}"
                
                gpu_info["devices"].append({
                    "id": i,
                    "device_name": device_name,
                    "name": torch.cuda.get_device_name(i),
                    "compute_capability": f"{props.major}.{props.minor}",
                    "total_memory_gb": round(total_memory, 2),
                    "allocated_memory_gb": round(allocated_memory, 2),
                    "cached_memory_gb": round(cached_memory, 2),
                    "free_memory_gb": round(total_memory - allocated_memory, 2),
                    "utilization": round((allocated_memory / total_memory * 100) if total_memory > 0 else 0, 1),
                    "current_tasks": {
                        "training": self.device_usage[device_name]["training"],
                        "inference": self.device_usage[device_name]["inference"]
                    }
                })
        
        return gpu_info
    
    def print_gpu_info(self):
        """æ‰“å°GPUä¿¡æ¯åˆ°æ§åˆ¶å°"""
        gpu_info = self.get_gpu_info()
        
        print("\n" + "="*70)
        print("ğŸš€ GPUç¡¬ä»¶ä¿¡æ¯")
        print("="*70)
        
        if not gpu_info["available"]:
            print("âŒ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
            return
        
        print(f"âœ… GPUå¯ç”¨")
        print(f"ğŸ“¦ CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
        print(f"ğŸ”§ PyTorchç‰ˆæœ¬: {gpu_info['pytorch_version']}")
        print(f"ğŸ¯ æ£€æµ‹åˆ° {gpu_info['count']} ä¸ªGPUè®¾å¤‡:")
        print()
        
        for device in gpu_info["devices"]:
            print(f"  GPU {device['id']} ({device['device_name']})")
            print(f"  â”œâ”€ å‹å·: {device['name']}")
            print(f"  â”œâ”€ Compute Capability: {device['compute_capability']}")
            print(f"  â”œâ”€ æ€»æ˜¾å­˜: {device['total_memory_gb']:.2f} GB")
            print(f"  â”œâ”€ å·²ç”¨æ˜¾å­˜: {device['allocated_memory_gb']:.2f} GB ({device['utilization']:.1f}%)")
            print(f"  â”œâ”€ ç©ºé—²æ˜¾å­˜: {device['free_memory_gb']:.2f} GB")
            print(f"  â””â”€ å½“å‰ä»»åŠ¡: è®­ç»ƒ={device['current_tasks']['training']}, æ¨ç†={device['current_tasks']['inference']}")
            print()
        
        print("="*70)
        print()
    
    def update_limits(self, config: Dict):
        """æ›´æ–°èµ„æºé™åˆ¶"""
        with self.lock:
            if "max_concurrent" in config:
                self.max_concurrent.update(config["max_concurrent"])
            logger.info(f"èµ„æºé™åˆ¶å·²æ›´æ–°: {self.max_concurrent}")


# å…¨å±€èµ„æºç®¡ç†å™¨å®ä¾‹
resource_manager = ResourceManager()

