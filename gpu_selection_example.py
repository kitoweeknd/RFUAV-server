"""
GPUè®¾å¤‡é€‰æ‹©ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é€‰æ‹©ä¸åŒçš„GPUè®¾å¤‡
"""
import requests
import time
from test_refactored_api import RFUAVClient


def print_separator(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)


def show_gpu_info(client):
    """æ˜¾ç¤ºGPUä¿¡æ¯"""
    print_separator("ğŸ” æŸ¥çœ‹GPUä¿¡æ¯")
    
    gpu_info = client.get_gpu_info()
    
    print(f"\nGPUå¯ç”¨: {gpu_info['available']}")
    if not gpu_info['available']:
        print("âŒ æœªæ£€æµ‹åˆ°GPU")
        return False
    
    print(f"CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
    print(f"PyTorchç‰ˆæœ¬: {gpu_info['pytorch_version']}")
    print(f"GPUæ•°é‡: {gpu_info['count']}\n")
    
    for device in gpu_info['devices']:
        print(f"GPU {device['id']} ({device['device_name']})")
        print(f"  å‹å·: {device['name']}")
        print(f"  æ€»æ˜¾å­˜: {device['total_memory_gb']:.2f} GB")
        print(f"  ç©ºé—²æ˜¾å­˜: {device['free_memory_gb']:.2f} GB")
        print(f"  ä½¿ç”¨ç‡: {device['utilization']:.1f}%")
        print(f"  å½“å‰ä»»åŠ¡: è®­ç»ƒ={device['current_tasks']['training']}, æ¨ç†={device['current_tasks']['inference']}")
        print()
    
    return True


def example_auto_selection(client):
    """ç¤ºä¾‹1: è‡ªåŠ¨é€‰æ‹©GPU"""
    print_separator("ğŸ“ ç¤ºä¾‹1: è‡ªåŠ¨é€‰æ‹©GPUï¼ˆæ¨èï¼‰")
    
    print("\nä½¿ç”¨ device='cuda' ä¼šè‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€å°çš„GPU")
    print("é€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼Œç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–èµ„æºåˆ†é…\n")
    
    # å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼Œè‡ªåŠ¨é€‰æ‹©GPU
    print("å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼ˆè‡ªåŠ¨é€‰æ‹©GPUï¼‰...")
    result = client.start_training(
        model="resnet18",
        num_classes=37,
        train_path="data/train",
        val_path="data/val",
        save_path="models/auto_gpu",
        batch_size=8,
        num_epochs=5,
        device="cuda"  # è‡ªåŠ¨é€‰æ‹©
    )
    
    task_id = result['task_id']
    print(f"âœ… ä»»åŠ¡å·²åˆ›å»º: {task_id}")
    print(f"   å®é™…ä½¿ç”¨è®¾å¤‡: {result.get('device', 'cuda')}")
    
    return task_id


def example_specific_gpu(client):
    """ç¤ºä¾‹2: æŒ‡å®šç‰¹å®šGPU"""
    print_separator("ğŸ“ ç¤ºä¾‹2: æŒ‡å®šç‰¹å®šGPU")
    
    print("\nä½¿ç”¨ device='cuda:0' æˆ– 'cuda:1' æŒ‡å®šå…·ä½“GPU")
    print("é€‚åˆéœ€è¦ç²¾ç¡®æ§åˆ¶èµ„æºçš„åœºæ™¯\n")
    
    # åœ¨GPU 0ä¸Šè®­ç»ƒ
    print("åœ¨GPU 0ä¸Šå¯åŠ¨è®­ç»ƒ...")
    result1 = client.start_training(
        model="resnet18",
        num_classes=37,
        train_path="data/train",
        val_path="data/val",
        save_path="models/gpu0",
        batch_size=8,
        num_epochs=5,
        device="cuda:0"  # æŒ‡å®šGPU 0
    )
    
    print(f"âœ… ä»»åŠ¡å·²åˆ›å»º (GPU 0): {result1['task_id']}")
    
    # åœ¨GPU 1ä¸Šæ¨ç†ï¼ˆå¦‚æœæœ‰å¤šä¸ªGPUï¼‰
    gpu_info = client.get_gpu_info()
    if gpu_info['count'] > 1:
        print("\nåœ¨GPU 1ä¸Šå¯åŠ¨æ¨ç†...")
        result2 = client.start_inference(
            cfg_path="configs/model.yaml",
            weight_path="models/best.pth",
            source_path="data/test",
            device="cuda:1"  # æŒ‡å®šGPU 1
        )
        print(f"âœ… ä»»åŠ¡å·²åˆ›å»º (GPU 1): {result2['task_id']}")
        return [result1['task_id'], result2['task_id']]
    else:
        print("\nâš ï¸ åªæœ‰ä¸€ä¸ªGPUï¼Œè·³è¿‡GPU 1ç¤ºä¾‹")
        return [result1['task_id']]


def example_multi_tasks(client):
    """ç¤ºä¾‹3: å¤šä»»åŠ¡å¹¶è¡Œ"""
    print_separator("ğŸ“ ç¤ºä¾‹3: å¤šä»»åŠ¡å¹¶è¡Œ")
    
    print("\nåŒæ—¶å¯åŠ¨å¤šä¸ªä»»åŠ¡ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ†é…åˆ°ä¸åŒGPU")
    print("å®ç°è´Ÿè½½å‡è¡¡\n")
    
    tasks = []
    
    # å¯åŠ¨3ä¸ªè®­ç»ƒä»»åŠ¡
    for i in range(3):
        print(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡ {i+1}...")
        result = client.start_training(
            model="resnet18",
            num_classes=37,
            train_path="data/train",
            val_path="data/val",
            save_path=f"models/parallel_{i}",
            batch_size=8,
            num_epochs=3,
            device="cuda",  # è‡ªåŠ¨é€‰æ‹©
            task_id=f"parallel_train_{i}"
        )
        tasks.append(result['task_id'])
        print(f"   ä»»åŠ¡ {i+1}: {result['task_id'][:8]}... -> {result.get('device', 'cuda')}")
        time.sleep(0.5)  # é¿å…è¿‡å¿«åˆ›å»º
    
    return tasks


def example_train_and_infer(client):
    """ç¤ºä¾‹4: è®­ç»ƒå’Œæ¨ç†åŒæ—¶è¿›è¡Œ"""
    print_separator("ğŸ“ ç¤ºä¾‹4: è®­ç»ƒå’Œæ¨ç†åŒæ—¶è¿›è¡Œ")
    
    print("\nåœ¨ä¸åŒGPUä¸ŠåŒæ—¶è¿è¡Œè®­ç»ƒå’Œæ¨ç†")
    print("å……åˆ†åˆ©ç”¨å¤šGPUèµ„æº\n")
    
    gpu_info = client.get_gpu_info()
    
    if gpu_info['count'] < 2:
        print("âš ï¸ éœ€è¦è‡³å°‘2ä¸ªGPUæ‰èƒ½æ¼”ç¤ºæ­¤ç¤ºä¾‹")
        return []
    
    tasks = []
    
    # åœ¨GPU 0ä¸Šè®­ç»ƒ
    print("åœ¨GPU 0ä¸Šå¯åŠ¨è®­ç»ƒ...")
    train_result = client.start_training(
        model="resnet18",
        num_classes=37,
        train_path="data/train",
        val_path="data/val",
        save_path="models/train_gpu0",
        device="cuda:0"
    )
    tasks.append(train_result['task_id'])
    print(f"âœ… è®­ç»ƒä»»åŠ¡: {train_result['task_id'][:8]}...")
    
    # åœ¨GPU 1ä¸Šæ¨ç†
    print("\nåœ¨GPU 1ä¸Šå¯åŠ¨æ¨ç†...")
    infer_result = client.start_inference(
        cfg_path="configs/model.yaml",
        weight_path="models/best.pth",
        source_path="data/test",
        device="cuda:1"
    )
    tasks.append(infer_result['task_id'])
    print(f"âœ… æ¨ç†ä»»åŠ¡: {infer_result['task_id'][:8]}...")
    
    return tasks


def show_resource_status(client):
    """æ˜¾ç¤ºèµ„æºä½¿ç”¨çŠ¶æ€"""
    print_separator("ğŸ“Š èµ„æºä½¿ç”¨çŠ¶æ€")
    
    resources = client.get_resources()
    
    print("\nè®¾å¤‡ä½¿ç”¨æƒ…å†µ:")
    for device, usage in resources['device_usage'].items():
        print(f"  {device}:")
        print(f"    è®­ç»ƒä»»åŠ¡: {usage['training']}")
        print(f"    æ¨ç†ä»»åŠ¡: {usage['inference']}")
    
    print("\nèµ„æºé™åˆ¶:")
    for device, limits in resources['limits'].items():
        print(f"  {device}:")
        print(f"    æœ€å¤§è®­ç»ƒä»»åŠ¡: {limits['training']}")
        print(f"    æœ€å¤§æ¨ç†ä»»åŠ¡: {limits['inference']}")
    
    print("\næ´»åŠ¨ä»»åŠ¡:")
    for device, tasks in resources['active_tasks'].items():
        if tasks:
            print(f"  {device}: {len(tasks)} ä¸ªä»»åŠ¡")
            for task in tasks:
                print(f"    - {task['id'][:8]}... ({task['type']})")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("  ğŸ® GPUè®¾å¤‡é€‰æ‹©ç¤ºä¾‹")
    print("  RFUAV Model Service V2.3")
    print("="*70)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = RFUAVClient("http://localhost:8000")
    
    # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
    try:
        health = client.health_check()
        print(f"\nâœ… æœåŠ¡çŠ¶æ€: {health['status']}")
        print(f"   ç‰ˆæœ¬: {health['version']}")
    except Exception as e:
        print(f"\nâŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡: {e}")
        print("   è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨: python app_refactored.py")
        return
    
    # 1. æ˜¾ç¤ºGPUä¿¡æ¯
    has_gpu = show_gpu_info(client)
    
    if not has_gpu:
        print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„GPUï¼Œç¤ºä¾‹å°†ä½¿ç”¨CPU")
    
    input("\næŒ‰Enterç»§ç»­ç¤ºä¾‹1...")
    
    # 2. ç¤ºä¾‹1: è‡ªåŠ¨é€‰æ‹©
    # example_auto_selection(client)
    # show_resource_status(client)
    
    # input("\næŒ‰Enterç»§ç»­ç¤ºä¾‹2...")
    
    # 3. ç¤ºä¾‹2: æŒ‡å®šGPU
    # example_specific_gpu(client)
    # show_resource_status(client)
    
    # input("\næŒ‰Enterç»§ç»­ç¤ºä¾‹3...")
    
    # 4. ç¤ºä¾‹3: å¤šä»»åŠ¡
    # example_multi_tasks(client)
    # show_resource_status(client)
    
    # input("\næŒ‰Enterç»§ç»­ç¤ºä¾‹4...")
    
    # 5. ç¤ºä¾‹4: è®­ç»ƒ+æ¨ç†
    # example_train_and_infer(client)
    # show_resource_status(client)
    
    print_separator("âœ… ç¤ºä¾‹å®Œæˆ")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. ä½¿ç”¨ 'cuda' è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜GPUï¼ˆæ¨èï¼‰")
    print("2. ä½¿ç”¨ 'cuda:0', 'cuda:1' æŒ‡å®šå…·ä½“GPU")
    print("3. å¤šGPUç¯å¢ƒä¸‹ç³»ç»Ÿè‡ªåŠ¨è´Ÿè½½å‡è¡¡")
    print("4. éšæ—¶é€šè¿‡ /api/v2/resources æŸ¥çœ‹èµ„æºçŠ¶æ€")
    print("\nè¯¦ç»†æ–‡æ¡£: GPU_SELECTION_GUIDE.md")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    main()


